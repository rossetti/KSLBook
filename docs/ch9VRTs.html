<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>9.2 Variance Reduction Techniques | Simulation Modeling using the Kotlin Simulation Library (KSL)</title>
  <meta name="description" content="A book that illustrates the basics of using the KSL. The output format for this book is bookdown::gitbook." />
  <meta name="generator" content="bookdown 0.40 and GitBook 2.6.7" />

  <meta property="og:title" content="9.2 Variance Reduction Techniques | Simulation Modeling using the Kotlin Simulation Library (KSL)" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="A book that illustrates the basics of using the KSL. The output format for this book is bookdown::gitbook." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="9.2 Variance Reduction Techniques | Simulation Modeling using the Kotlin Simulation Library (KSL)" />
  
  <meta name="twitter:description" content="A book that illustrates the basics of using the KSL. The output format for this book is bookdown::gitbook." />
  

<meta name="author" content="Manuel D. Rossetti" />


<meta name="date" content="2025-09-22" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ch9BootStrapping.html"/>
<link rel="next" href="ch9GMVRVs.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Simulation Modeling using the Kotlin Simulation Library (KSL)</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="release-history.html"><a href="release-history.html"><i class="fa fa-check"></i>Release History</a></li>
<li class="chapter" data-level="" data-path="ksl-project-page.html"><a href="ksl-project-page.html"><i class="fa fa-check"></i>KSL Project Page</a></li>
<li class="chapter" data-level="" data-path="book-support-files.html"><a href="book-support-files.html"><i class="fa fa-check"></i>Book Support Files</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
<li class="chapter" data-level="" data-path="intended-audience.html"><a href="intended-audience.html"><i class="fa fa-check"></i>Intended Audience</a></li>
<li class="chapter" data-level="" data-path="organization-of-the-book.html"><a href="organization-of-the-book.html"><i class="fa fa-check"></i>Organization of the Book</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-the-author.html"><a href="about-the-author.html"><i class="fa fa-check"></i>About the Author</a></li>
<li class="chapter" data-level="1" data-path="ch1.html"><a href="ch1.html"><i class="fa fa-check"></i><b>1</b> Simulation Modeling</a>
<ul>
<li class="chapter" data-level="1.1" data-path="simulation-modeling.html"><a href="simulation-modeling.html"><i class="fa fa-check"></i><b>1.1</b> Simulation Modeling</a></li>
<li class="chapter" data-level="1.2" data-path="why-simulate.html"><a href="why-simulate.html"><i class="fa fa-check"></i><b>1.2</b> Why Simulate?</a></li>
<li class="chapter" data-level="1.3" data-path="types-of-systems-and-simulation-models.html"><a href="types-of-systems-and-simulation-models.html"><i class="fa fa-check"></i><b>1.3</b> Types of Systems and Simulation Models</a></li>
<li class="chapter" data-level="1.4" data-path="simulation-descriptive-or-prescriptive-modeling.html"><a href="simulation-descriptive-or-prescriptive-modeling.html"><i class="fa fa-check"></i><b>1.4</b> Simulation: Descriptive or Prescriptive Modeling?</a></li>
<li class="chapter" data-level="1.5" data-path="randomness-in-simulation.html"><a href="randomness-in-simulation.html"><i class="fa fa-check"></i><b>1.5</b> Randomness in Simulation</a></li>
<li class="chapter" data-level="1.6" data-path="simulation-languages.html"><a href="simulation-languages.html"><i class="fa fa-check"></i><b>1.6</b> Simulation Languages</a></li>
<li class="chapter" data-level="1.7" data-path="ch1secsimMeth.html"><a href="ch1secsimMeth.html"><i class="fa fa-check"></i><b>1.7</b> Simulation Methodology</a></li>
<li class="chapter" data-level="1.8" data-path="overview-of-the-kotlin-simulation-library.html"><a href="overview-of-the-kotlin-simulation-library.html"><i class="fa fa-check"></i><b>1.8</b> Overview of the Kotlin Simulation Library</a></li>
<li class="chapter" data-level="1.9" data-path="simulation-as-a-profession.html"><a href="simulation-as-a-profession.html"><i class="fa fa-check"></i><b>1.9</b> Simulation as a Profession</a></li>
<li class="chapter" data-level="1.10" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>1.10</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ch2rng.html"><a href="ch2rng.html"><i class="fa fa-check"></i><b>2</b> Modeling Randomness</a>
<ul>
<li class="chapter" data-level="2.1" data-path="ch2generator.html"><a href="ch2generator.html"><i class="fa fa-check"></i><b>2.1</b> Random Number Generator</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="ch2generator.html"><a href="ch2generator.html#ch2randompkg"><i class="fa fa-check"></i><b>2.1.1</b> Random Package</a></li>
<li class="chapter" data-level="2.1.2" data-path="ch2generator.html"><a href="ch2generator.html#ch2creatingStreams"><i class="fa fa-check"></i><b>2.1.2</b> Creating and Using Streams</a></li>
<li class="chapter" data-level="2.1.3" data-path="ch2generator.html"><a href="ch2generator.html#ch2crn"><i class="fa fa-check"></i><b>2.1.3</b> Common Random Numbers</a></li>
<li class="chapter" data-level="2.1.4" data-path="ch2generator.html"><a href="ch2generator.html#ch2antitheticStreams"><i class="fa fa-check"></i><b>2.1.4</b> Creating and Using Antithetic Streams</a></li>
<li class="chapter" data-level="2.1.5" data-path="ch2generator.html"><a href="ch2generator.html#ch2rnFAQ"><i class="fa fa-check"></i><b>2.1.5</b> Frequently Asked Questions about Random Numbers</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="rvg.html"><a href="rvg.html"><i class="fa fa-check"></i><b>2.2</b> Random Variate Generation</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="rvg.html"><a href="rvg.html#basic-random-variate-generation"><i class="fa fa-check"></i><b>2.2.1</b> Basic Random Variate Generation</a></li>
<li class="chapter" data-level="2.2.2" data-path="rvg.html"><a href="rvg.html#rvg_dists"><i class="fa fa-check"></i><b>2.2.2</b> Continuous and Discrete Random Variables</a></li>
<li class="chapter" data-level="2.2.3" data-path="rvg.html"><a href="rvg.html#rvguse"><i class="fa fa-check"></i><b>2.2.3</b> Creating and Using Random Variables</a></li>
<li class="chapter" data-level="2.2.4" data-path="rvg.html"><a href="rvg.html#functions-of-random-variables"><i class="fa fa-check"></i><b>2.2.4</b> Functions of Random Variables</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="probModels.html"><a href="probModels.html"><i class="fa fa-check"></i><b>2.3</b> Probability Distribution Models</a></li>
<li class="chapter" data-level="2.4" data-path="distFitting.html"><a href="distFitting.html"><i class="fa fa-check"></i><b>2.4</b> Distribution Fitting Using the KSL</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="distFitting.html"><a href="distFitting.html#estimating-distribution-parameters"><i class="fa fa-check"></i><b>2.4.1</b> Estimating Distribution Parameters</a></li>
<li class="chapter" data-level="2.4.2" data-path="distFitting.html"><a href="distFitting.html#continuous-distribution-recommendation-framework"><i class="fa fa-check"></i><b>2.4.2</b> Continuous Distribution Recommendation Framework</a></li>
<li class="chapter" data-level="2.4.3" data-path="distFitting.html"><a href="distFitting.html#discrete-distribution-framework"><i class="fa fa-check"></i><b>2.4.3</b> Discrete Distribution Framework</a></li>
<li class="chapter" data-level="2.4.4" data-path="distFitting.html"><a href="distFitting.html#pdfmexamples"><i class="fa fa-check"></i><b>2.4.4</b> Illustrative Examples from Appendix @ref(appidm)</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>2.5</b> Summary</a></li>
<li class="chapter" data-level="2.6" data-path="exercises-1.html"><a href="exercises-1.html"><i class="fa fa-check"></i><b>2.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="mcm.html"><a href="mcm.html"><i class="fa fa-check"></i><b>3</b> Monte Carlo Methods</a>
<ul>
<li class="chapter" data-level="3.1" data-path="kslStatistics.html"><a href="kslStatistics.html"><i class="fa fa-check"></i><b>3.1</b> Collecting Statistics</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="kslStatistics.html"><a href="kslStatistics.html#creating-and-using-a-statistic"><i class="fa fa-check"></i><b>3.1.1</b> Creating and Using a Statistic</a></li>
<li class="chapter" data-level="3.1.2" data-path="kslStatistics.html"><a href="kslStatistics.html#histFreq"><i class="fa fa-check"></i><b>3.1.2</b> Histograms and Frequencies</a></li>
<li class="chapter" data-level="3.1.3" data-path="kslStatistics.html"><a href="kslStatistics.html#ch3batchStats"><i class="fa fa-check"></i><b>3.1.3</b> Batch Statistics</a></li>
<li class="chapter" data-level="3.1.4" data-path="kslStatistics.html"><a href="kslStatistics.html#ch3StatSummary"><i class="fa fa-check"></i><b>3.1.4</b> Statistics Summary</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="ssMC.html"><a href="ssMC.html"><i class="fa fa-check"></i><b>3.2</b> Simple Monte Carlo Integration</a></li>
<li class="chapter" data-level="3.3" data-path="ch3StatReview.html"><a href="ch3StatReview.html"><i class="fa fa-check"></i><b>3.3</b> Review of Statistical Concepts</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="ch3StatReview.html"><a href="ch3StatReview.html#point-estimates-and-confidence-intervals"><i class="fa fa-check"></i><b>3.3.1</b> Point Estimates and Confidence Intervals</a></li>
<li class="chapter" data-level="3.3.2" data-path="ch3StatReview.html"><a href="ch3StatReview.html#ch3SampleSize"><i class="fa fa-check"></i><b>3.3.2</b> Sample Size Determination</a></li>
<li class="chapter" data-level="3.3.3" data-path="ch3StatReview.html"><a href="ch3StatReview.html#determining-the-sample-size-for-a-monte-carlo-simulation-experiment"><i class="fa fa-check"></i><b>3.3.3</b> Determining the Sample Size for a Monte Carlo Simulation Experiment</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="craps.html"><a href="craps.html"><i class="fa fa-check"></i><b>3.4</b> Simulating the Game of Craps</a></li>
<li class="chapter" data-level="3.5" data-path="the-news-vendor-problem.html"><a href="the-news-vendor-problem.html"><i class="fa fa-check"></i><b>3.5</b> The News Vendor Problem</a></li>
<li class="chapter" data-level="3.6" data-path="ch3InsProcess.html"><a href="ch3InsProcess.html"><i class="fa fa-check"></i><b>3.6</b> A Simple Inspection Process</a></li>
<li class="chapter" data-level="3.7" data-path="ch3SAN.html"><a href="ch3SAN.html"><i class="fa fa-check"></i><b>3.7</b> Stochastic Activity Networks</a></li>
<li class="chapter" data-level="3.8" data-path="mcmExperiments.html"><a href="mcmExperiments.html"><i class="fa fa-check"></i><b>3.8</b> Monte-Carlo Experiments</a></li>
<li class="chapter" data-level="3.9" data-path="summary-1.html"><a href="summary-1.html"><i class="fa fa-check"></i><b>3.9</b> Summary</a></li>
<li class="chapter" data-level="3.10" data-path="exercises-2.html"><a href="exercises-2.html"><i class="fa fa-check"></i><b>3.10</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="introDEDS.html"><a href="introDEDS.html"><i class="fa fa-check"></i><b>4</b> Introduction to Discrete Event Modeling</a>
<ul>
<li class="chapter" data-level="4.1" data-path="introDEDSdeds.html"><a href="introDEDSdeds.html"><i class="fa fa-check"></i><b>4.1</b> Discrete-Event Dynamic Systems</a></li>
<li class="chapter" data-level="4.2" data-path="HowDEDSClockWorks.html"><a href="HowDEDSClockWorks.html"><i class="fa fa-check"></i><b>4.2</b> How the Discrete-Event Clock Works</a></li>
<li class="chapter" data-level="4.3" data-path="QHandExample.html"><a href="QHandExample.html"><i class="fa fa-check"></i><b>4.3</b> Simulating a Queueing System By Hand</a></li>
<li class="chapter" data-level="4.4" data-path="introDEDSdedsKSL.html"><a href="introDEDSdedsKSL.html"><i class="fa fa-check"></i><b>4.4</b> Modeling DEDS in the KSL</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="introDEDSdedsKSL.html"><a href="introDEDSdedsKSL.html#event-scheduling"><i class="fa fa-check"></i><b>4.4.1</b> Event Scheduling</a></li>
<li class="chapter" data-level="4.4.2" data-path="introDEDSdedsKSL.html"><a href="introDEDSdedsKSL.html#introDEDSschedExamples"><i class="fa fa-check"></i><b>4.4.2</b> Simple Event Scheduling Examples</a></li>
<li class="chapter" data-level="4.4.3" data-path="introDEDSdedsKSL.html"><a href="introDEDSdedsKSL.html#introDEDSUpDown"><i class="fa fa-check"></i><b>4.4.3</b> Up and Down Component Example</a></li>
<li class="chapter" data-level="4.4.4" data-path="introDEDSdedsKSL.html"><a href="introDEDSdedsKSL.html#introDEDSPharmacy"><i class="fa fa-check"></i><b>4.4.4</b> Modeling a Simple Queueing System</a></li>
<li class="chapter" data-level="4.4.5" data-path="introDEDSdedsKSL.html"><a href="introDEDSdedsKSL.html#more-details-about-the-pharmacy-model-implementation"><i class="fa fa-check"></i><b>4.4.5</b> More Details About the Pharmacy Model Implementation</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="enhancing-the-drive-through-pharmacy-model.html"><a href="enhancing-the-drive-through-pharmacy-model.html"><i class="fa fa-check"></i><b>4.5</b> Enhancing the Drive Through Pharmacy Model</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="enhancing-the-drive-through-pharmacy-model.html"><a href="enhancing-the-drive-through-pharmacy-model.html#modeling-a-simple-resource"><i class="fa fa-check"></i><b>4.5.1</b> Modeling a Simple Resource</a></li>
<li class="chapter" data-level="4.5.2" data-path="enhancing-the-drive-through-pharmacy-model.html"><a href="enhancing-the-drive-through-pharmacy-model.html#modeling-a-queue-with-statistical-collection"><i class="fa fa-check"></i><b>4.5.2</b> Modeling a Queue with Statistical Collection</a></li>
<li class="chapter" data-level="4.5.3" data-path="enhancing-the-drive-through-pharmacy-model.html"><a href="enhancing-the-drive-through-pharmacy-model.html#modeling-a-repeating-event-pattern"><i class="fa fa-check"></i><b>4.5.3</b> Modeling a Repeating Event Pattern</a></li>
<li class="chapter" data-level="4.5.4" data-path="enhancing-the-drive-through-pharmacy-model.html"><a href="enhancing-the-drive-through-pharmacy-model.html#collecting-more-detailed-statistics"><i class="fa fa-check"></i><b>4.5.4</b> Collecting More Detailed Statistics</a></li>
<li class="chapter" data-level="4.5.5" data-path="enhancing-the-drive-through-pharmacy-model.html"><a href="enhancing-the-drive-through-pharmacy-model.html#implementing-the-enhanced-pharmacy-model"><i class="fa fa-check"></i><b>4.5.5</b> Implementing the Enhanced Pharmacy Model</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="DTPExpanded.html"><a href="DTPExpanded.html"><i class="fa fa-check"></i><b>4.6</b> More Drive Through Fun</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="DTPExpanded.html"><a href="DTPExpanded.html#modeling-a-resource-with-a-waiting-line"><i class="fa fa-check"></i><b>4.6.1</b> Modeling a Resource with a Waiting Line</a></li>
<li class="chapter" data-level="4.6.2" data-path="DTPExpanded.html"><a href="DTPExpanded.html#modeling-the-tandem-queue-of-example-refexmexch4tandemq"><i class="fa fa-check"></i><b>4.6.2</b> Modeling the Tandem Queue of Example @ref(exm:exCh4TandemQ)</a></li>
<li class="chapter" data-level="4.6.3" data-path="DTPExpanded.html"><a href="DTPExpanded.html#modeling-with-the-station-package"><i class="fa fa-check"></i><b>4.6.3</b> Modeling with the Station Package</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="introDEDSSummary.html"><a href="introDEDSSummary.html"><i class="fa fa-check"></i><b>4.7</b> Summary</a></li>
<li class="chapter" data-level="4.8" data-path="exercises-3.html"><a href="exercises-3.html"><i class="fa fa-check"></i><b>4.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="simoa.html"><a href="simoa.html"><i class="fa fa-check"></i><b>5</b> Analyzing and Accessing Simulation Output</a>
<ul>
<li class="chapter" data-level="5.1" data-path="simoadatatypes.html"><a href="simoadatatypes.html"><i class="fa fa-check"></i><b>5.1</b> Types of Statistical Variables</a></li>
<li class="chapter" data-level="5.2" data-path="simoasimtypes.html"><a href="simoasimtypes.html"><i class="fa fa-check"></i><b>5.2</b> Types of Simulation With Respect To Output Analysis</a></li>
<li class="chapter" data-level="5.3" data-path="simoafinhorizon.html"><a href="simoafinhorizon.html"><i class="fa fa-check"></i><b>5.3</b> Analysis of Finite Horizon Simulations</a></li>
<li class="chapter" data-level="5.4" data-path="simoafinhorizonex.html"><a href="simoafinhorizonex.html"><i class="fa fa-check"></i><b>5.4</b> Capturing Output for a Simple Finite Horizon Simulation</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="simoafinhorizonex.html"><a href="simoafinhorizonex.html#simoaCapture"><i class="fa fa-check"></i><b>5.4.1</b> KSL Functionality for Capturing Statistical Results</a></li>
<li class="chapter" data-level="5.4.2" data-path="simoafinhorizonex.html"><a href="simoafinhorizonex.html#additional-remarks"><i class="fa fa-check"></i><b>5.4.2</b> Additional Remarks</a></li>
<li class="chapter" data-level="5.4.3" data-path="simoafinhorizonex.html"><a href="simoafinhorizonex.html#querying-the-ksl-database"><i class="fa fa-check"></i><b>5.4.3</b> Querying the KSL Database</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="simoaseqsampling.html"><a href="simoaseqsampling.html"><i class="fa fa-check"></i><b>5.5</b> Sequential Sampling for Finite Horizon Simulations</a></li>
<li class="chapter" data-level="5.6" data-path="simoainfhorizon.html"><a href="simoainfhorizon.html"><i class="fa fa-check"></i><b>5.6</b> Analysis of Infinite Horizon Simulations</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="simoainfhorizon.html"><a href="simoainfhorizon.html#simoainfhorizoninitialbias"><i class="fa fa-check"></i><b>5.6.1</b> Assessing the Effect of Initial Conditions</a></li>
<li class="chapter" data-level="5.6.2" data-path="simoainfhorizon.html"><a href="simoainfhorizon.html#simoainfhorizonrepDeletion"><i class="fa fa-check"></i><b>5.6.2</b> Performing the Method of Replication-Deletion</a></li>
<li class="chapter" data-level="5.6.3" data-path="simoainfhorizon.html"><a href="simoainfhorizon.html#simoainfhorizonbatchmeans"><i class="fa fa-check"></i><b>5.6.3</b> The Method of Batch Means</a></li>
<li class="chapter" data-level="5.6.4" data-path="simoainfhorizon.html"><a href="simoainfhorizon.html#simoainfhorizonjslbatching"><i class="fa fa-check"></i><b>5.6.4</b> Performing the Method of Batch Means</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="simoacomparingSystems.html"><a href="simoacomparingSystems.html"><i class="fa fa-check"></i><b>5.7</b> Comparing System Configurations</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="simoacomparingSystems.html"><a href="simoacomparingSystems.html#simoacomparingSystems:two"><i class="fa fa-check"></i><b>5.7.1</b> Comparing Two Systems</a></li>
<li class="chapter" data-level="5.7.2" data-path="simoacomparingSystems.html"><a href="simoacomparingSystems.html#simoacomparingSystemsMCB"><i class="fa fa-check"></i><b>5.7.2</b> Concepts for Comparing Systems</a></li>
<li class="chapter" data-level="5.7.3" data-path="simoacomparingSystems.html"><a href="simoacomparingSystems.html#multiple-comparison-with-the-best-procedures-mcb"><i class="fa fa-check"></i><b>5.7.3</b> Multiple Comparison with the Best Procedures (MCB)</a></li>
<li class="chapter" data-level="5.7.4" data-path="simoacomparingSystems.html"><a href="simoacomparingSystems.html#ch5Screening"><i class="fa fa-check"></i><b>5.7.4</b> Screening Procedures</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="ch5Scenarios.html"><a href="ch5Scenarios.html"><i class="fa fa-check"></i><b>5.8</b> Simulating Many Scenarios</a>
<ul>
<li class="chapter" data-level="5.8.1" data-path="ch5Scenarios.html"><a href="ch5Scenarios.html#controlAntns"><i class="fa fa-check"></i><b>5.8.1</b> Control Annotations</a></li>
<li class="chapter" data-level="5.8.2" data-path="ch5Scenarios.html"><a href="ch5Scenarios.html#rvParameters"><i class="fa fa-check"></i><b>5.8.2</b> Random Variable Parameters</a></li>
<li class="chapter" data-level="5.8.3" data-path="ch5Scenarios.html"><a href="ch5Scenarios.html#kslScenarios"><i class="fa fa-check"></i><b>5.8.3</b> Setting Up and Running Multiple Scenarios</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="simoasummary.html"><a href="simoasummary.html"><i class="fa fa-check"></i><b>5.9</b> Summary</a></li>
<li class="chapter" data-level="5.10" data-path="exercises-4.html"><a href="exercises-4.html"><i class="fa fa-check"></i><b>5.10</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="processview.html"><a href="processview.html"><i class="fa fa-check"></i><b>6</b> Process View Modeling Using the KSL</a>
<ul>
<li class="chapter" data-level="6.1" data-path="ch6Entities.html"><a href="ch6Entities.html"><i class="fa fa-check"></i><b>6.1</b> What are Entities?</a></li>
<li class="chapter" data-level="6.2" data-path="pvIntro.html"><a href="pvIntro.html"><i class="fa fa-check"></i><b>6.2</b> The Process View</a></li>
<li class="chapter" data-level="6.3" data-path="understanding-ksl-processes-and-entities.html"><a href="understanding-ksl-processes-and-entities.html"><i class="fa fa-check"></i><b>6.3</b> Understanding KSL Processes and Entities</a></li>
<li class="chapter" data-level="6.4" data-path="examples-of-process-modeling.html"><a href="examples-of-process-modeling.html"><i class="fa fa-check"></i><b>6.4</b> Examples of Process Modeling</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="examples-of-process-modeling.html"><a href="examples-of-process-modeling.html#holding-entities-in-a-queue-holdqueue-class"><i class="fa fa-check"></i><b>6.4.1</b> Holding Entities in a Queue: <code>HoldQueue</code> Class</a></li>
<li class="chapter" data-level="6.4.2" data-path="examples-of-process-modeling.html"><a href="examples-of-process-modeling.html#signaling-entities"><i class="fa fa-check"></i><b>6.4.2</b> Signaling Entities</a></li>
<li class="chapter" data-level="6.4.3" data-path="examples-of-process-modeling.html"><a href="examples-of-process-modeling.html#understanding-blocking-queues"><i class="fa fa-check"></i><b>6.4.3</b> Understanding Blocking Queues</a></li>
<li class="chapter" data-level="6.4.4" data-path="examples-of-process-modeling.html"><a href="examples-of-process-modeling.html#allowing-entities-to-wait-for-a-process"><i class="fa fa-check"></i><b>6.4.4</b> Allowing Entities to Wait for a Process</a></li>
<li class="chapter" data-level="6.4.5" data-path="examples-of-process-modeling.html"><a href="examples-of-process-modeling.html#processinteraction"><i class="fa fa-check"></i><b>6.4.5</b> Process Interaction</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="modeling-a-stem-career-mixer.html"><a href="modeling-a-stem-career-mixer.html"><i class="fa fa-check"></i><b>6.5</b> Modeling a STEM Career Mixer</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="modeling-a-stem-career-mixer.html"><a href="modeling-a-stem-career-mixer.html#conceptualizing-the-system"><i class="fa fa-check"></i><b>6.5.1</b> Conceptualizing the System</a></li>
<li class="chapter" data-level="6.5.2" data-path="modeling-a-stem-career-mixer.html"><a href="modeling-a-stem-career-mixer.html#implementing-the-stem-mixer-model"><i class="fa fa-check"></i><b>6.5.2</b> Implementing the STEM Mixer Model</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="the-tie-dye-t-shirt-model.html"><a href="the-tie-dye-t-shirt-model.html"><i class="fa fa-check"></i><b>6.6</b> The Tie-Dye T-Shirt Model</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="the-tie-dye-t-shirt-model.html"><a href="the-tie-dye-t-shirt-model.html#ch4:TieDyeTShirtsSub1"><i class="fa fa-check"></i><b>6.6.1</b> Implementing the Tie-Dye T-Shirt Model</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="summary-2.html"><a href="summary-2.html"><i class="fa fa-check"></i><b>6.7</b> Summary</a></li>
<li class="chapter" data-level="6.8" data-path="exercises-5.html"><a href="exercises-5.html"><i class="fa fa-check"></i><b>6.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ch7AdvModeling.html"><a href="ch7AdvModeling.html"><i class="fa fa-check"></i><b>7</b> Advanced Event and Process View Modeling</a>
<ul>
<li class="chapter" data-level="7.1" data-path="modeling-with-processes-and-resources.html"><a href="modeling-with-processes-and-resources.html"><i class="fa fa-check"></i><b>7.1</b> Modeling with Processes and Resources</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="modeling-with-processes-and-resources.html"><a href="modeling-with-processes-and-resources.html#modeling-space-with-resources"><i class="fa fa-check"></i><b>7.1.1</b> Modeling Space with Resources</a></li>
<li class="chapter" data-level="7.1.2" data-path="modeling-with-processes-and-resources.html"><a href="modeling-with-processes-and-resources.html#secResourcePools"><i class="fa fa-check"></i><b>7.1.2</b> Resource Pools</a></li>
<li class="chapter" data-level="7.1.3" data-path="modeling-with-processes-and-resources.html"><a href="modeling-with-processes-and-resources.html#secTestAndRepair"><i class="fa fa-check"></i><b>7.1.3</b> Computer Test and Repair Shop Example</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="modeling-non-stationary-systems.html"><a href="modeling-non-stationary-systems.html"><i class="fa fa-check"></i><b>7.2</b> Modeling Non-Stationary Systems</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="modeling-non-stationary-systems.html"><a href="modeling-non-stationary-systems.html#ch7secNSPP"><i class="fa fa-check"></i><b>7.2.1</b> Non-Stationary Arrival Processes</a></li>
<li class="chapter" data-level="7.2.2" data-path="modeling-non-stationary-systems.html"><a href="modeling-non-stationary-systems.html#modeling-resources-under-non-stationary-conditions"><i class="fa fa-check"></i><b>7.2.2</b> Modeling Resources Under Non-Stationary Conditions</a></li>
<li class="chapter" data-level="7.2.3" data-path="modeling-non-stationary-systems.html"><a href="modeling-non-stationary-systems.html#enhancing-the-stem-career-mixer-model"><i class="fa fa-check"></i><b>7.2.3</b> Enhancing the STEM Career Mixer Model</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="examples-of-advanced-event-models.html"><a href="examples-of-advanced-event-models.html"><i class="fa fa-check"></i><b>7.3</b> Examples of Advanced Event Models</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="examples-of-advanced-event-models.html"><a href="examples-of-advanced-event-models.html#ch6s1sb4sub3"><i class="fa fa-check"></i><b>7.3.1</b> Modeling Balking and Reneging</a></li>
<li class="chapter" data-level="7.3.2" data-path="examples-of-advanced-event-models.html"><a href="examples-of-advanced-event-models.html#rqModel"><i class="fa fa-check"></i><b>7.3.2</b> Modeling a Reorder Point, Reorder Quantity Inventory Policy</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="ch7VandV.html"><a href="ch7VandV.html"><i class="fa fa-check"></i><b>7.4</b> Applying Queueing Theory Results to Verify and Validate a Simulation</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="ch7VandV.html"><a href="ch7VandV.html#analyzing-the-preparation-station"><i class="fa fa-check"></i><b>7.4.1</b> Analyzing the Preparation Station</a></li>
<li class="chapter" data-level="7.4.2" data-path="ch7VandV.html"><a href="ch7VandV.html#analyzing-the-build-lines"><i class="fa fa-check"></i><b>7.4.2</b> Analyzing the Build Lines</a></li>
<li class="chapter" data-level="7.4.3" data-path="ch7VandV.html"><a href="ch7VandV.html#analyzing-the-packaging-station"><i class="fa fa-check"></i><b>7.4.3</b> Analyzing the Packaging Station</a></li>
<li class="chapter" data-level="7.4.4" data-path="ch7VandV.html"><a href="ch7VandV.html#analyzing-the-palletizing-station"><i class="fa fa-check"></i><b>7.4.4</b> Analyzing the Palletizing Station</a></li>
<li class="chapter" data-level="7.4.5" data-path="ch7VandV.html"><a href="ch7VandV.html#analyzing-the-total-system-time"><i class="fa fa-check"></i><b>7.4.5</b> Analyzing the Total System Time</a></li>
<li class="chapter" data-level="7.4.6" data-path="ch7VandV.html"><a href="ch7VandV.html#other-issues-for-verification-and-validation"><i class="fa fa-check"></i><b>7.4.6</b> Other Issues for Verification and Validation</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="summary-3.html"><a href="summary-3.html"><i class="fa fa-check"></i><b>7.5</b> Summary</a></li>
<li class="chapter" data-level="7.6" data-path="exercises-6.html"><a href="exercises-6.html"><i class="fa fa-check"></i><b>7.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="chEntityMovement.html"><a href="chEntityMovement.html"><i class="fa fa-check"></i><b>8</b> Modeling Entity Movement</a>
<ul>
<li class="chapter" data-level="8.1" data-path="secRCT.html"><a href="secRCT.html"><i class="fa fa-check"></i><b>8.1</b> Resource Constrained Transfer</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="secRCT.html"><a href="secRCT.html#secTestRepairRCT"><i class="fa fa-check"></i><b>8.1.1</b> Test and Repair with Resource Constrained Transfer</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="movableResources.html"><a href="movableResources.html"><i class="fa fa-check"></i><b>8.2</b> Constrained Transfer with Movable Resources</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="movableResources.html"><a href="movableResources.html#spatial-models"><i class="fa fa-check"></i><b>8.2.1</b> Spatial Models</a></li>
<li class="chapter" data-level="8.2.2" data-path="movableResources.html"><a href="movableResources.html#secMovableResources"><i class="fa fa-check"></i><b>8.2.2</b> Movable Resources</a></li>
<li class="chapter" data-level="8.2.3" data-path="movableResources.html"><a href="movableResources.html#secTQMWM"><i class="fa fa-check"></i><b>8.2.3</b> Tandem Queue Model With Movement</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="secTestAndRepairMovableResources.html"><a href="secTestAndRepairMovableResources.html"><i class="fa fa-check"></i><b>8.3</b> Modeling the Test and Repair System with Movable Resources</a></li>
<li class="chapter" data-level="8.4" data-path="secConveyors.html"><a href="secConveyors.html"><i class="fa fa-check"></i><b>8.4</b> Modeling Systems with Conveyors</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="secConveyors.html"><a href="secConveyors.html#ksl-conveyor-constructs"><i class="fa fa-check"></i><b>8.4.1</b> KSL Conveyor Constructs</a></li>
<li class="chapter" data-level="8.4.2" data-path="secConveyors.html"><a href="secConveyors.html#tandem-queue-system-with-conveyors"><i class="fa fa-check"></i><b>8.4.2</b> Tandem Queue System with Conveyors</a></li>
<li class="chapter" data-level="8.4.3" data-path="secConveyors.html"><a href="secConveyors.html#secTestAndRepairConveyors"><i class="fa fa-check"></i><b>8.4.3</b> Test and Repair via Conveyors</a></li>
<li class="chapter" data-level="8.4.4" data-path="secConveyors.html"><a href="secConveyors.html#secMiscConveyor"><i class="fa fa-check"></i><b>8.4.4</b> Miscellaneous Concepts in Conveyor Modeling</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="summary-of-new-concepts.html"><a href="summary-of-new-concepts.html"><i class="fa fa-check"></i><b>8.5</b> Summary of New Concepts</a></li>
<li class="chapter" data-level="8.6" data-path="exercises-7.html"><a href="exercises-7.html"><i class="fa fa-check"></i><b>8.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="ch9AdvMC.html"><a href="ch9AdvMC.html"><i class="fa fa-check"></i><b>9</b> Advanced Monte Carlo Methods</a>
<ul>
<li class="chapter" data-level="9.1" data-path="ch9BootStrapping.html"><a href="ch9BootStrapping.html"><i class="fa fa-check"></i><b>9.1</b> Bootstrap Methods</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="ch9BootStrapping.html"><a href="ch9BootStrapping.html#bootstrapping-using-the-ksl"><i class="fa fa-check"></i><b>9.1.1</b> Bootstrapping Using the KSL</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="ch9VRTs.html"><a href="ch9VRTs.html"><i class="fa fa-check"></i><b>9.2</b> Variance Reduction Techniques</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="ch9VRTs.html"><a href="ch9VRTs.html#common-random-numbers-crn"><i class="fa fa-check"></i><b>9.2.1</b> Common Random Numbers (CRN)</a></li>
<li class="chapter" data-level="9.2.2" data-path="ch9VRTs.html"><a href="ch9VRTs.html#antithetic-variates-av"><i class="fa fa-check"></i><b>9.2.2</b> Antithetic Variates (AV)</a></li>
<li class="chapter" data-level="9.2.3" data-path="ch9VRTs.html"><a href="ch9VRTs.html#indirect-estimation"><i class="fa fa-check"></i><b>9.2.3</b> Indirect Estimation</a></li>
<li class="chapter" data-level="9.2.4" data-path="ch9VRTs.html"><a href="ch9VRTs.html#control-variates-cv"><i class="fa fa-check"></i><b>9.2.4</b> Control Variates (CV)</a></li>
<li class="chapter" data-level="9.2.5" data-path="ch9VRTs.html"><a href="ch9VRTs.html#stratified-and-post-stratified-sampling"><i class="fa fa-check"></i><b>9.2.5</b> Stratified and Post Stratified Sampling</a></li>
<li class="chapter" data-level="9.2.6" data-path="ch9VRTs.html"><a href="ch9VRTs.html#conditional-expectation-ce"><i class="fa fa-check"></i><b>9.2.6</b> Conditional Expectation (CE)</a></li>
<li class="chapter" data-level="9.2.7" data-path="ch9VRTs.html"><a href="ch9VRTs.html#importance-sampling-is"><i class="fa fa-check"></i><b>9.2.7</b> Importance Sampling (IS)</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="ch9GMVRVs.html"><a href="ch9GMVRVs.html"><i class="fa fa-check"></i><b>9.3</b> Generating Multi-Variate and Correlated Random Variates</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="ch9GMVRVs.html"><a href="ch9GMVRVs.html#generating-from-a-bivariate-normal-distribution"><i class="fa fa-check"></i><b>9.3.1</b> Generating from a Bivariate Normal Distribution</a></li>
<li class="chapter" data-level="9.3.2" data-path="ch9GMVRVs.html"><a href="ch9GMVRVs.html#copulas-and-multi-variate-generation-methods"><i class="fa fa-check"></i><b>9.3.2</b> Copulas and Multi-variate Generation Methods</a></li>
<li class="chapter" data-level="9.3.3" data-path="ch9GMVRVs.html"><a href="ch9GMVRVs.html#autocorrelated-generation"><i class="fa fa-check"></i><b>9.3.3</b> Autocorrelated Generation</a></li>
<li class="chapter" data-level="9.3.4" data-path="ch9GMVRVs.html"><a href="ch9GMVRVs.html#ch9MCMC"><i class="fa fa-check"></i><b>9.3.4</b> Introduction to Markov Chain Monte Carlo Methods</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="summary-4.html"><a href="summary-4.html"><i class="fa fa-check"></i><b>9.4</b> Summary</a></li>
<li class="chapter" data-level="9.5" data-path="exercises-8.html"><a href="exercises-8.html"><i class="fa fa-check"></i><b>9.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="ch10eo.html"><a href="ch10eo.html"><i class="fa fa-check"></i><b>10</b> Experimental Design and Simulation Optimization Methods</a>
<ul>
<li class="chapter" data-level="10.1" data-path="ch10doe.html"><a href="ch10doe.html"><i class="fa fa-check"></i><b>10.1</b> Experimental Design Methods</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="ch10doe.html"><a href="ch10doe.html#ch10factors"><i class="fa fa-check"></i><b>10.1.1</b> Defining Factors</a></li>
<li class="chapter" data-level="10.1.2" data-path="ch10doe.html"><a href="ch10doe.html#ch10designs"><i class="fa fa-check"></i><b>10.1.2</b> Defining Experimental Designs</a></li>
<li class="chapter" data-level="10.1.3" data-path="ch10doe.html"><a href="ch10doe.html#ch10exp"><i class="fa fa-check"></i><b>10.1.3</b> Executing a Designed Experiment</a></li>
<li class="chapter" data-level="10.1.4" data-path="ch10doe.html"><a href="ch10doe.html#ch10analysis"><i class="fa fa-check"></i><b>10.1.4</b> Analyzing a Designed Experiment</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="ch10simOpt.html"><a href="ch10simOpt.html"><i class="fa fa-check"></i><b>10.2</b> Simulation Optimization Methods</a></li>
<li class="chapter" data-level="10.3" data-path="ch10ProbDefn.html"><a href="ch10ProbDefn.html"><i class="fa fa-check"></i><b>10.3</b> Defining Simulation Optimization Problems</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="ch10ProbDefn.html"><a href="ch10ProbDefn.html#specifying-penalty-functions"><i class="fa fa-check"></i><b>10.3.1</b> Specifying Penalty Functions</a></li>
<li class="chapter" data-level="10.3.2" data-path="ch10ProbDefn.html"><a href="ch10ProbDefn.html#inputvariables"><i class="fa fa-check"></i><b>10.3.2</b> Defining Input Variables</a></li>
<li class="chapter" data-level="10.3.3" data-path="ch10ProbDefn.html"><a href="ch10ProbDefn.html#linear-and-functional-constraints"><i class="fa fa-check"></i><b>10.3.3</b> Linear and Functional Constraints</a></li>
<li class="chapter" data-level="10.3.4" data-path="ch10ProbDefn.html"><a href="ch10ProbDefn.html#response-constraints"><i class="fa fa-check"></i><b>10.3.4</b> Response Constraints</a></li>
<li class="chapter" data-level="10.3.5" data-path="ch10ProbDefn.html"><a href="ch10ProbDefn.html#ch10ProbDefnExample"><i class="fa fa-check"></i><b>10.3.5</b> Setting Up a Problem Definition</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="representing-the-simulation-oracle.html"><a href="representing-the-simulation-oracle.html"><i class="fa fa-check"></i><b>10.4</b> Representing the Simulation Oracle</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="representing-the-simulation-oracle.html"><a href="representing-the-simulation-oracle.html#requesting-simulation-evaluations"><i class="fa fa-check"></i><b>10.4.1</b> Requesting Simulation Evaluations</a></li>
<li class="chapter" data-level="10.4.2" data-path="representing-the-simulation-oracle.html"><a href="representing-the-simulation-oracle.html#representing-model-inputs"><i class="fa fa-check"></i><b>10.4.2</b> Representing Model Inputs</a></li>
<li class="chapter" data-level="10.4.3" data-path="representing-the-simulation-oracle.html"><a href="representing-the-simulation-oracle.html#representing-model-outputs"><i class="fa fa-check"></i><b>10.4.3</b> Representing Model Outputs</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="characterizing-simulation-optimization-algorithms.html"><a href="characterizing-simulation-optimization-algorithms.html"><i class="fa fa-check"></i><b>10.5</b> Characterizing Simulation Optimization Algorithms</a>
<ul>
<li class="chapter" data-level="10.5.1" data-path="characterizing-simulation-optimization-algorithms.html"><a href="characterizing-simulation-optimization-algorithms.html#structure-of-the-iterative-process"><i class="fa fa-check"></i><b>10.5.1</b> Structure of the Iterative Process</a></li>
<li class="chapter" data-level="10.5.2" data-path="characterizing-simulation-optimization-algorithms.html"><a href="characterizing-simulation-optimization-algorithms.html#solver-plug-in-behavior"><i class="fa fa-check"></i><b>10.5.2</b> Solver Plug-in Behavior</a></li>
<li class="chapter" data-level="10.5.3" data-path="characterizing-simulation-optimization-algorithms.html"><a href="characterizing-simulation-optimization-algorithms.html#miscellaneous-concepts-in-the-solver-class"><i class="fa fa-check"></i><b>10.5.3</b> Miscellaneous Concepts in the <code>Solver</code> Class</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="implemented-solvers.html"><a href="implemented-solvers.html"><i class="fa fa-check"></i><b>10.6</b> Implemented Solvers</a>
<ul>
<li class="chapter" data-level="10.6.1" data-path="implemented-solvers.html"><a href="implemented-solvers.html#simulated-annealing"><i class="fa fa-check"></i><b>10.6.1</b> Simulated Annealing</a></li>
<li class="chapter" data-level="10.6.2" data-path="implemented-solvers.html"><a href="implemented-solvers.html#cross-entropy"><i class="fa fa-check"></i><b>10.6.2</b> Cross Entropy</a></li>
<li class="chapter" data-level="10.6.3" data-path="implemented-solvers.html"><a href="implemented-solvers.html#r-spline"><i class="fa fa-check"></i><b>10.6.3</b> R-SPLINE</a></li>
<li class="chapter" data-level="10.6.4" data-path="implemented-solvers.html"><a href="implemented-solvers.html#randomized-restart-solvers"><i class="fa fa-check"></i><b>10.6.4</b> Randomized Restart Solvers</a></li>
</ul></li>
<li class="chapter" data-level="10.7" data-path="secApplySolver.html"><a href="secApplySolver.html"><i class="fa fa-check"></i><b>10.7</b> Using a Solver on a Problem</a>
<ul>
<li class="chapter" data-level="10.7.1" data-path="secApplySolver.html"><a href="secApplySolver.html#applyCE"><i class="fa fa-check"></i><b>10.7.1</b> Illustrating the Cross-Entropy Solver</a></li>
<li class="chapter" data-level="10.7.2" data-path="secApplySolver.html"><a href="secApplySolver.html#illustrating-simulated-annealing-with-random-restarts"><i class="fa fa-check"></i><b>10.7.2</b> Illustrating Simulated Annealing with Random Restarts</a></li>
</ul></li>
<li class="chapter" data-level="10.8" data-path="summary-5.html"><a href="summary-5.html"><i class="fa fa-check"></i><b>10.8</b> Summary</a></li>
<li class="chapter" data-level="10.9" data-path="exercises-9.html"><a href="exercises-9.html"><i class="fa fa-check"></i><b>10.9</b> Exercises</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="appRNRV.html"><a href="appRNRV.html"><i class="fa fa-check"></i><b>A</b> Generating Pseudo-Random Numbers and Random Variates</a>
<ul>
<li class="chapter" data-level="A.1" data-path="appRNRVPRN.html"><a href="appRNRVPRN.html"><i class="fa fa-check"></i><b>A.1</b> Pseudo Random Numbers</a>
<ul>
<li class="chapter" data-level="A.1.1" data-path="appRNRVPRN.html"><a href="appRNRVPRN.html#appRNRVRNGs"><i class="fa fa-check"></i><b>A.1.1</b> Random Number Generators</a></li>
</ul></li>
<li class="chapter" data-level="A.2" data-path="appRNRVs.html"><a href="appRNRVs.html"><i class="fa fa-check"></i><b>A.2</b> Generating Random Variates from Distributions</a>
<ul>
<li class="chapter" data-level="A.2.1" data-path="appRNRVs.html"><a href="appRNRVs.html#inverse-transform-method"><i class="fa fa-check"></i><b>A.2.1</b> Inverse Transform Method</a></li>
<li class="chapter" data-level="A.2.2" data-path="appRNRVs.html"><a href="appRNRVs.html#convolution"><i class="fa fa-check"></i><b>A.2.2</b> Convolution</a></li>
<li class="chapter" data-level="A.2.3" data-path="appRNRVs.html"><a href="appRNRVs.html#acceptancerejection"><i class="fa fa-check"></i><b>A.2.3</b> Acceptance/Rejection</a></li>
<li class="chapter" data-level="A.2.4" data-path="appRNRVs.html"><a href="appRNRVs.html#AppRNRVsubsecMTSRV"><i class="fa fa-check"></i><b>A.2.4</b> Mixture Distributions, Truncated Distributions, and Shifted Random Variables</a></li>
</ul></li>
<li class="chapter" data-level="A.3" data-path="summary-6.html"><a href="summary-6.html"><i class="fa fa-check"></i><b>A.3</b> Summary</a></li>
<li class="chapter" data-level="A.4" data-path="exercises-10.html"><a href="exercises-10.html"><i class="fa fa-check"></i><b>A.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="appidm.html"><a href="appidm.html"><i class="fa fa-check"></i><b>B</b> Probability Distribution Modeling</a>
<ul>
<li class="chapter" data-level="B.1" data-path="appidmsecrvPD.html"><a href="appidmsecrvPD.html"><i class="fa fa-check"></i><b>B.1</b> Random Variables and Probability Distributions</a></li>
<li class="chapter" data-level="B.2" data-path="appidmsecMDD.html"><a href="appidmsecMDD.html"><i class="fa fa-check"></i><b>B.2</b> Modeling with Discrete Distributions</a></li>
<li class="chapter" data-level="B.3" data-path="appidmsecfitDiscrete.html"><a href="appidmsecfitDiscrete.html"><i class="fa fa-check"></i><b>B.3</b> Fitting Discrete Distributions</a>
<ul>
<li class="chapter" data-level="B.3.1" data-path="appidmsecfitDiscrete.html"><a href="appidmsecfitDiscrete.html#AppDisFitPoissonFit"><i class="fa fa-check"></i><b>B.3.1</b> Fitting a Poisson Distribution</a></li>
<li class="chapter" data-level="B.3.2" data-path="appidmsecfitDiscrete.html"><a href="appidmsecfitDiscrete.html#visualizing-the-data"><i class="fa fa-check"></i><b>B.3.2</b> Visualizing the Data</a></li>
<li class="chapter" data-level="B.3.3" data-path="appidmsecfitDiscrete.html"><a href="appidmsecfitDiscrete.html#estimating-the-rate-parameter-for-the-poisson-distribution"><i class="fa fa-check"></i><b>B.3.3</b> Estimating the Rate Parameter for the Poisson Distribution</a></li>
<li class="chapter" data-level="B.3.4" data-path="appidmsecfitDiscrete.html"><a href="appidmsecfitDiscrete.html#chi-squared-goodness-of-fit-test-for-poisson-distribution"><i class="fa fa-check"></i><b>B.3.4</b> Chi-Squared Goodness of Fit Test for Poisson Distribution</a></li>
<li class="chapter" data-level="B.3.5" data-path="appidmsecfitDiscrete.html"><a href="appidmsecfitDiscrete.html#subsubchisqGOF"><i class="fa fa-check"></i><b>B.3.5</b> Chi-Squared Goodness of Fit Test</a></li>
<li class="chapter" data-level="B.3.6" data-path="appidmsecfitDiscrete.html"><a href="appidmsecfitDiscrete.html#using-the-fitdistrplus-r-package-on-discrete-data"><i class="fa fa-check"></i><b>B.3.6</b> Using the fitdistrplus R Package on Discrete Data</a></li>
<li class="chapter" data-level="B.3.7" data-path="appidmsecfitDiscrete.html"><a href="appidmsecfitDiscrete.html#fitting-a-discrete-empirical-distribution"><i class="fa fa-check"></i><b>B.3.7</b> Fitting a Discrete Empirical Distribution</a></li>
</ul></li>
<li class="chapter" data-level="B.4" data-path="appidmsecMCD.html"><a href="appidmsecMCD.html"><i class="fa fa-check"></i><b>B.4</b> Modeling with Continuous Distributions</a></li>
<li class="chapter" data-level="B.5" data-path="appidmsecfitContinuous.html"><a href="appidmsecfitContinuous.html"><i class="fa fa-check"></i><b>B.5</b> Fitting Continuous Distributions</a>
<ul>
<li class="chapter" data-level="B.5.1" data-path="appidmsecfitContinuous.html"><a href="appidmsecfitContinuous.html#appidmsubsecvisualizedata"><i class="fa fa-check"></i><b>B.5.1</b> Visualizing the Data</a></li>
<li class="chapter" data-level="B.5.2" data-path="appidmsecfitContinuous.html"><a href="appidmsecfitContinuous.html#appidmsubsecstatsdata"><i class="fa fa-check"></i><b>B.5.2</b> Statistically Summarize the Data</a></li>
<li class="chapter" data-level="B.5.3" data-path="appidmsecfitContinuous.html"><a href="appidmsecfitContinuous.html#appidmsubsechypothDist"><i class="fa fa-check"></i><b>B.5.3</b> Hypothesizing and Testing a Distribution</a></li>
<li class="chapter" data-level="B.5.4" data-path="appidmsecfitContinuous.html"><a href="appidmsecfitContinuous.html#kolmogorov-smirnov-test"><i class="fa fa-check"></i><b>B.5.4</b> Kolmogorov-Smirnov Test</a></li>
<li class="chapter" data-level="B.5.5" data-path="appidmsecfitContinuous.html"><a href="appidmsecfitContinuous.html#appidmsubsecvisFit"><i class="fa fa-check"></i><b>B.5.5</b> Visualizing the Fit</a></li>
<li class="chapter" data-level="B.5.6" data-path="appidmsecfitContinuous.html"><a href="appidmsecfitContinuous.html#appidms2sb3"><i class="fa fa-check"></i><b>B.5.6</b> Using the Input Analyzer</a></li>
</ul></li>
<li class="chapter" data-level="B.6" data-path="appdistfittestU01.html"><a href="appdistfittestU01.html"><i class="fa fa-check"></i><b>B.6</b> Testing Uniform (0,1) Pseudo-Random Numbers</a>
<ul>
<li class="chapter" data-level="B.6.1" data-path="appdistfittestU01.html"><a href="appdistfittestU01.html#chi-squared-goodness-of-fit-tests-for-pseudo-random-numbers"><i class="fa fa-check"></i><b>B.6.1</b> Chi-Squared Goodness of Fit Tests for Pseudo-Random Numbers</a></li>
<li class="chapter" data-level="B.6.2" data-path="appdistfittestU01.html"><a href="appdistfittestU01.html#higher-dimensional-chi-squared-test"><i class="fa fa-check"></i><b>B.6.2</b> Higher Dimensional Chi-Squared Test</a></li>
<li class="chapter" data-level="B.6.3" data-path="appdistfittestU01.html"><a href="appdistfittestU01.html#kolmogorov-smirnov-test-for-pseudo-random-numbers"><i class="fa fa-check"></i><b>B.6.3</b> Kolmogorov-Smirnov Test for Pseudo-Random Numbers</a></li>
<li class="chapter" data-level="B.6.4" data-path="appdistfittestU01.html"><a href="appdistfittestU01.html#testing-for-independence-and-patterns-in-pseudo-random-numbers"><i class="fa fa-check"></i><b>B.6.4</b> Testing for Independence and Patterns in Pseudo-Random Numbers</a></li>
</ul></li>
<li class="chapter" data-level="B.7" data-path="appdistfitidms2sb4.html"><a href="appdistfitidms2sb4.html"><i class="fa fa-check"></i><b>B.7</b> Additional Distribution Modeling Concepts</a></li>
<li class="chapter" data-level="B.8" data-path="appidmSummary.html"><a href="appidmSummary.html"><i class="fa fa-check"></i><b>B.8</b> Summary</a></li>
<li class="chapter" data-level="B.9" data-path="exercises-11.html"><a href="exercises-11.html"><i class="fa fa-check"></i><b>B.9</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="appqtAndInvT.html"><a href="appqtAndInvT.html"><i class="fa fa-check"></i><b>C</b> Queueing Theory</a>
<ul>
<li class="chapter" data-level="C.1" data-path="appqts1.html"><a href="appqts1.html"><i class="fa fa-check"></i><b>C.1</b> Single Line Queueing Stations</a>
<ul>
<li class="chapter" data-level="C.1.1" data-path="appqts1.html"><a href="appqts1.html#queueing-notation"><i class="fa fa-check"></i><b>C.1.1</b> Queueing Notation</a></li>
<li class="chapter" data-level="C.1.2" data-path="appqts1.html"><a href="appqts1.html#littles-formula"><i class="fa fa-check"></i><b>C.1.2</b> Littles Formula</a></li>
<li class="chapter" data-level="C.1.3" data-path="appqts1.html"><a href="appqts1.html#appqts1sb1"><i class="fa fa-check"></i><b>C.1.3</b> Deriving Formulas for Markovian Single Queue Systems</a></li>
</ul></li>
<li class="chapter" data-level="C.2" data-path="appqts1sb2.html"><a href="appqts1sb2.html"><i class="fa fa-check"></i><b>C.2</b> Examples and Applications of Queueing Analysis</a>
<ul>
<li class="chapter" data-level="C.2.1" data-path="appqts1sb2.html"><a href="appqts1sb2.html#infinite-queue-examples"><i class="fa fa-check"></i><b>C.2.1</b> Infinite Queue Examples</a></li>
<li class="chapter" data-level="C.2.2" data-path="appqts1sb2.html"><a href="appqts1sb2.html#finite-queue-examples"><i class="fa fa-check"></i><b>C.2.2</b> Finite Queue Examples</a></li>
</ul></li>
<li class="chapter" data-level="C.3" data-path="appqts1sb3.html"><a href="appqts1sb3.html"><i class="fa fa-check"></i><b>C.3</b> Non-Markovian Queues and Approximations</a></li>
<li class="chapter" data-level="C.4" data-path="appqtsecformulas.html"><a href="appqtsecformulas.html"><i class="fa fa-check"></i><b>C.4</b> Summary of Queueing Formulas</a>
<ul>
<li class="chapter" data-level="C.4.1" data-path="appqtsecformulas.html"><a href="appqtsecformulas.html#mm1-queue"><i class="fa fa-check"></i><b>C.4.1</b> M/M/1 Queue</a></li>
<li class="chapter" data-level="C.4.2" data-path="appqtsecformulas.html"><a href="appqtsecformulas.html#mmc-queue"><i class="fa fa-check"></i><b>C.4.2</b> M/M/c Queue</a></li>
<li class="chapter" data-level="C.4.3" data-path="appqtsecformulas.html"><a href="appqtsecformulas.html#mmck-queue"><i class="fa fa-check"></i><b>C.4.3</b> M/M/c/k Queue</a></li>
<li class="chapter" data-level="C.4.4" data-path="appqtsecformulas.html"><a href="appqtsecformulas.html#mgcc-queue"><i class="fa fa-check"></i><b>C.4.4</b> M/G/c/c Queue</a></li>
<li class="chapter" data-level="C.4.5" data-path="appqtsecformulas.html"><a href="appqtsecformulas.html#mm1k-queue"><i class="fa fa-check"></i><b>C.4.5</b> M/M/1/k Queue</a></li>
<li class="chapter" data-level="C.4.6" data-path="appqtsecformulas.html"><a href="appqtsecformulas.html#mmck-queue-1"><i class="fa fa-check"></i><b>C.4.6</b> M/M/c/k Queue</a></li>
<li class="chapter" data-level="C.4.7" data-path="appqtsecformulas.html"><a href="appqtsecformulas.html#mm1kk-queue"><i class="fa fa-check"></i><b>C.4.7</b> M/M/1/k/k Queue</a></li>
<li class="chapter" data-level="C.4.8" data-path="appqtsecformulas.html"><a href="appqtsecformulas.html#mmckk-queue"><i class="fa fa-check"></i><b>C.4.8</b> M/M/c/k/k Queue</a></li>
</ul></li>
<li class="chapter" data-level="C.5" data-path="exercises-12.html"><a href="exercises-12.html"><i class="fa fa-check"></i><b>C.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="D" data-path="appUtilities.html"><a href="appUtilities.html"><i class="fa fa-check"></i><b>D</b> KSL Utility Packages</a>
<ul>
<li class="chapter" data-level="D.1" data-path="the-outputdirectory-class-and-ksl-object.html"><a href="the-outputdirectory-class-and-ksl-object.html"><i class="fa fa-check"></i><b>D.1</b> The <code>OutputDirectory</code> Class and <code>KSL</code> Object</a></li>
<li class="chapter" data-level="D.2" data-path="logging-options.html"><a href="logging-options.html"><i class="fa fa-check"></i><b>D.2</b> Logging Options</a></li>
<li class="chapter" data-level="D.3" data-path="the-kslfileutil-object.html"><a href="the-kslfileutil-object.html"><i class="fa fa-check"></i><b>D.3</b> The <code>KSLFileUtil</code> Object</a></li>
<li class="chapter" data-level="D.4" data-path="appDCSVEtc.html"><a href="appDCSVEtc.html"><i class="fa fa-check"></i><b>D.4</b> CSV, Excel, and Tabular Data Files</a></li>
<li class="chapter" data-level="D.5" data-path="dfUtil.html"><a href="dfUtil.html"><i class="fa fa-check"></i><b>D.5</b> The <code>DataFrameUtil</code> Object</a></li>
<li class="chapter" data-level="D.6" data-path="ksl-database-utilities.html"><a href="ksl-database-utilities.html"><i class="fa fa-check"></i><b>D.6</b> KSL Database Utilities</a></li>
<li class="chapter" data-level="D.7" data-path="appUtilitiesArrays.html"><a href="appUtilitiesArrays.html"><i class="fa fa-check"></i><b>D.7</b> Array Utilities</a></li>
<li class="chapter" data-level="D.8" data-path="appPlotting.html"><a href="appPlotting.html"><i class="fa fa-check"></i><b>D.8</b> KSL Plotting Utilities</a></li>
<li class="chapter" data-level="D.9" data-path="appUtilitiesMisc.html"><a href="appUtilitiesMisc.html"><i class="fa fa-check"></i><b>D.9</b> Miscellaneous Utilities</a></li>
</ul></li>
<li class="chapter" data-level="E" data-path="distributions.html"><a href="distributions.html"><i class="fa fa-check"></i><b>E</b> Distributions</a>
<ul>
<li class="chapter" data-level="E.1" data-path="appDiscreteDistributions.html"><a href="appDiscreteDistributions.html"><i class="fa fa-check"></i><b>E.1</b> Discrete Distrbutions</a></li>
<li class="chapter" data-level="E.2" data-path="appContinuousDistributions.html"><a href="appContinuousDistributions.html"><i class="fa fa-check"></i><b>E.2</b> Continuous Distrbutions</a></li>
</ul></li>
<li class="chapter" data-level="F" data-path="appStatTables.html"><a href="appStatTables.html"><i class="fa fa-check"></i><b>F</b> Statistical Tables</a></li>
<li class="chapter" data-level="G" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>G</b> References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Simulation Modeling using the Kotlin Simulation Library (KSL)</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ch9VRTs" class="section level2 hasAnchor" number="9.2">
<h2><span class="header-section-number">9.2</span> Variance Reduction Techniques<a href="ch9VRTs.html#ch9VRTs" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A variance reduction technique (VRT) represents a sampling strategy that has the overall goal of finding an estimator that will have lower variance than the typical estimator based on straightforward random sampling. The objective of a VRT is to reduce the theoretical population variance of the <strong>sampling</strong> distribution associated with an estimator <span class="math inline">\(\hat{\theta}\)</span>, of some population parameter <span class="math inline">\(\theta\)</span>. For example, in many instances, we are interested in the population mean, <span class="math inline">\(\theta\)</span>, and thus the standard estimator that is proposed is the sample mean <span class="math inline">\(\hat{\theta} = \bar{Y}\)</span>. Suppose we have the following IID observations <span class="math inline">\((Y_1, Y_2, \cdots, Y_n)\)</span> with <span class="math inline">\(E[Y_i]=\theta\)</span> and <span class="math inline">\(\text{Var}[Y_i] = \sigma^{2}_{Y}\)</span>. Then the standard (or so called crude) estimator for <span class="math inline">\(\theta\)</span> is <span class="math inline">\(\bar{Y}\)</span> and the variance of the estimator is</p>
<p><span class="math display">\[
\text{Var}[\bar{Y}]=\text{Var}\bigg[\frac{1}{n}\sum_{i=1}^{n}Y_i \bigg] = \frac{1}{n^2}\sum_{i=1}^{n}\text{Var}[Y_i]=\frac{n \,\text{Var}[Y_i]}{n^2} = \frac{\text{Var}[Y_i]}{n}=\frac{\sigma^2_{Y}}{n}
\]</span></p>
<p>When applying a VRT, the goal is to do better than <span class="math inline">\(\text{Var}[\bar{Y}]\)</span>. While it may be obvious, it is worth stating that the simplest variance reduction technique is to use a larger sample. Consider estimating the population mean, <span class="math inline">\(\theta\)</span> based on a random sample using:</p>
<p><span class="math display">\[
\bar{Y}(n) = \frac{1}{n}\sum_{i=1}^{n}Y_i
\]</span>
Here, we have emphasized that the estimator, <span class="math inline">\(\bar{Y}(n)\)</span> is a function of the sample size, <span class="math inline">\(n\)</span>. So consider two estimators, <span class="math inline">\(\hat{\theta}_1 = \bar{Y}(n_1)\)</span> and <span class="math inline">\(\hat{\theta}_2 =\bar{Y}(n_2)\)</span> where <span class="math inline">\(n_1 &lt; n_2\)</span>. Thus, we will have that:</p>
<p><span class="math display">\[
\text{Var}[\bar{Y}(n_1)] = \text{Var}[\hat{\theta}_1] = \frac{\sigma^2_{Y}}{n_1}
\]</span></p>
<p>and,</p>
<p><span class="math display">\[
\text{Var}[\bar{Y}(n_2)] = \text{Var}[\hat{\theta}_2] = \frac{\sigma^2_{Y}}{n_2}
\]</span>
and thus because <span class="math inline">\(n_1 &lt; n_2\)</span>, we have that <span class="math inline">\(\text{Var}[\bar{Y}(n_2)] &lt; \text{Var}[\bar{Y}(n_1)]\)</span>. Thus, we have two estimators <span class="math inline">\(\hat{\theta}_1\)</span> and <span class="math inline">\(\hat{\theta}_2\)</span> where the variance of one estimator is smaller than the variance of another. In this particular case, both of the estimators are unbiased, and not considering the extra time that may be required to sample the larger sample, we would prefer the estimator <span class="math inline">\(\hat{\theta}_2\)</span> over the estimator <span class="math inline">\(\hat{\theta}_1\)</span> because <span class="math inline">\(\hat{\theta}_2\)</span> has lower variance. That is, the variance of its <strong>sampling</strong> distribution is lower. As noted, we have to ignore the possible extra computation time needed for <span class="math inline">\(\hat{\theta}_2\)</span>. In what follows, we will generally ignore computational issues, and in most of the VRT methods that will be presented, the assumption will be very reasonable. However, there is also the effort needed to implement a more complicated sampling method than straightforward random sampling. In some modeling situations, this extra implementation effort should not be ignored when considering the possible use of a VRT.</p>
<p>The important thing to remember is that the variance reduction refers to reducing the population variance of the <em>estimator</em> of <span class="math inline">\(\theta\)</span>. The parameter of interest does not have to be the mean. The parameter <span class="math inline">\(\theta\)</span> can be any parameter of the underlying population. For example, we might try to estimate the median, quantile, or the population variance <span class="math inline">\(\sigma^{2}_{Y}\)</span>. One important point to remember, variance reduction methods do not affect the variability of the process (population). That is <span class="math inline">\(\sigma^{2}_{Y}\)</span> does not change. When applying a VRT, we are changing the variance associated with the <em>sampling distribution</em> of the estimator, <span class="math inline">\(\hat{\theta}\)</span>.</p>
<p>The major types of variance reduction techniques to be discussed in this section, include:</p>
<ol style="list-style-type: decimal">
<li>Manipulating randomness
<ol style="list-style-type: lower-alpha">
<li>Common Random Numbers (CRN)</li>
<li>Antithetic Variates (AV)</li>
</ol></li>
<li>Exploiting process knowledge
<ol style="list-style-type: lower-alpha">
<li>Indirect Estimation (IE)</li>
<li>Control Variates (CV)</li>
<li>Stratified and Post Stratified Sampling (PS)</li>
<li>Conditional Expectation (CE)</li>
<li>Importance Sampling (IS)</li>
</ol></li>
</ol>
<p>Manipulating randomness refers to the judicious use of the random number streams. Exploiting process knowledge refers to the general intuition that more information reduces uncertainty.</p>
<div id="common-random-numbers-crn" class="section level3 hasAnchor" number="9.2.1">
<h3><span class="header-section-number">9.2.1</span> Common Random Numbers (CRN)<a href="ch9VRTs.html#common-random-numbers-crn" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In Section <a href="simoacomparingSystems.html#simoacomparingSystemstwoDep">5.7.1.2</a>, we already discussed the use of common random numbers when comparing two systems. Here we will review the key results and mention a couple of implementation issues.</p>
<p>CRN exploits the idea that if each alternative experiences the same randomness, then the difference between the two alternatives will be because of a true underlying difference, rather than because of the randomness. Define <span class="math inline">\(\theta = \theta_1 - \theta_2\)</span> as the parameter of interest, and define <span class="math inline">\(D_i = X_i - Y_i\)</span> as the difference between the <span class="math inline">\(i^{th}\)</span> pairs of performance observations, for <span class="math inline">\(i=1,2, \cdots, n\)</span>, where <span class="math inline">\(n\)</span> is the number of observations. The point estimator is <span class="math inline">\(\hat{\theta} = \bar{D} = \bar{X} - \bar{Y}\)</span>, with interval estimator:</p>
<p><span class="math display">\[
\bar{D} \pm t_{1-\alpha/2,n-1}\frac{s_D}{\sqrt{n}}
\]</span></p>
<p>By considering the variance of <span class="math inline">\(\bar{D} = \bar{X} - \bar{Y}\)</span>, we have that,</p>
<p><span class="math display">\[
\text{Var}[\bar{D}]=\frac{\text{Var}[D_i]}{n}=\frac{1}{n}\text{Var}[X_i - Y_i]=\frac{1}{n}\Bigg[\text{Var}[X_i]+\text{Var}[Y_i]-2\text{cov}[X_i,Y_i] \Bigg]
\]</span></p>
<p>And, therefore,</p>
<p><span class="math display">\[
\text{Var}[\bar{D}]=\frac{\sigma^{2}_{X}}{n} +\frac{\sigma^{2}_{Y}}{n} - 2\rho_{XY}\sigma_{X}\sigma_{Y}
\]</span></p>
<p>If the random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, then <span class="math inline">\(\rho_{XY}= 0\)</span> and:
<span class="math display">\[
V_{IND} = \text{Var}[\bar{D}]=\frac{\sigma^{2}_{X}}{n} +\frac{\sigma^{2}_{Y}}{n}
\]</span></p>
<p>In analyzing the worth of a VRT, we will need to derive the variance of the estimator. Thus, deriving the variance for the CRN and the independent (IND) estimators, we have:</p>
<p><span class="math display">\[
V_{CRN} = V_{IND} - 2\rho_{XY}\sigma_{X}\sigma_{Y}
\]</span></p>
<p><span class="math display">\[
V_{IND} - V_{CRN} = 2\rho_{XY}\sigma_{X}\sigma_{Y}
\]</span></p>
<p>This implies that if there is positive correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, <span class="math inline">\(\rho_{XY} \ge 0\)</span>, we will have:</p>
<p><span class="math display">\[
V_{IND} - V_{CRN} \ge 0
\]</span></p>
<p><span class="math display">\[
V_{CRN} \le V_{IND}
\]</span></p>
<p>Therefore, the variance of the CRN estimator should be smaller than the variance of the crude estimator if there is positive correlation. If we can induce a positive correlation within the pairs <span class="math inline">\((X_i, Y_i)\)</span>, then when we use</p>
<p><span class="math display">\[
\hat{\theta} = \frac{1}{n}\sum_{i=1}^{n}D_i = \frac{1}{n}\sum_{i=1}^{n}(X_i - Y_i)
\]</span></p>
<p>we will have a variance reduction (a more precise estimate). It is important to note that the sampling distribution of <span class="math inline">\(\hat{\theta}\)</span> is different <em>because</em> the pairs <span class="math inline">\((X_i, Y_i)\)</span> were sampled in a <em>dependent</em> manner.</p>
<p>So, CRN will work when we can induce a positive correlation within the pairs <span class="math inline">\((X_i, Y_i)\)</span> and it will backfire (cause an increase in the variance) if there is a negative correlation within the pairs <span class="math inline">\((X_i, Y_i)\)</span>. In the case of simple Monte Carlo, it should be clear that we can induce positive correlation between <span class="math inline">\(X_i\)</span> and <span class="math inline">\(Y_i\)</span> by using the same random numbers during the generation process. Suppose <span class="math inline">\(X_i \sim F_{X}(x)\)</span> and <span class="math inline">\(Y_i \sim F_{Y}(y)\)</span> and we have the inverse transform functions for <span class="math inline">\(F_X\)</span> and <span class="math inline">\(F_Y\)</span>. Then, to generate <span class="math inline">\(X_i\)</span> and <span class="math inline">\(Y_i\)</span> we use:</p>
<p><span class="math display">\[
U_i \sim U(0,1)\\
X_i = F_{X}^{-1}(U_i)\\
Y_i = F_{Y}^{-1}(U_i)\\
\]</span>
Thus, if <span class="math inline">\(U_i\)</span> is large, then because the inverse CDF, <span class="math inline">\(F^{-1}(p)\)</span> is a monotone function, <span class="math inline">\(X_i\)</span> will be large <em>and</em> <span class="math inline">\(Y_i\)</span> will be large. In addition, if <span class="math inline">\(U_i\)</span> is small, then <span class="math inline">\(X_i\)</span> will be small <em>and</em> <span class="math inline">\(Y_i\)</span> will be small. Thus, there will be positive correlation. In this simple case of <span class="math inline">\(D_i = X_i - Y_i\)</span> the monotone relationship will be maintained; however, in a general discrete event dynamic simulation modeling context, the situation is much more complex.</p>
<p>For example, suppose we are estimating the waiting time in bank open from 9 am to 5 pm, <span class="math inline">\(\theta\)</span>. Here we have an arrival distribution, <span class="math inline">\(F_A(x)\)</span>, and a service distribution, <span class="math inline">\(F_S(x)\)</span>, and the number of tellers that govern the underlying stochastic processes. Suppose we want to compare the performance (in terms of the average waiting time) for system 1, <span class="math inline">\(\theta_1\)</span> to the average waiting time for system 2, <span class="math inline">\(\theta_2\)</span>, where the arrival rate of system 1, <span class="math inline">\(\lambda_1\)</span> is higher than the arrival rate for system 2, <span class="math inline">\(\lambda_2\)</span> and the service distributions remain the same, as well as the number of tellers.</p>
<p>Now, suspending our analytical thinking about which system actually should have the higher waiting time, we run simulation experiments to compare, <span class="math inline">\(\hat{\theta}_1\)</span> to <span class="math inline">\(\hat{\theta}_2\)</span>. Assuming that we run two experiments, each with <span class="math inline">\(r\)</span> as the number of replications, and <span class="math inline">\(m_{r}^{i}\)</span> as the number of observed customers in replication <span class="math inline">\(r\)</span> for system <span class="math inline">\(i\)</span>, where <span class="math inline">\(W_{kj}^{i}\)</span> is the observed waiting time for system <span class="math inline">\(i\)</span> for customer <span class="math inline">\(k\)</span> in replication <span class="math inline">\(j\)</span>, we have that our estimators of performance for <span class="math inline">\(i=1, 2\)</span> to be:</p>
<p><span class="math display">\[
\hat{\theta}_i = \frac{1}{r}\sum_{j=1}^{r}\bar{W}_{j}^{i}
\]</span></p>
<p>where,</p>
<p><span class="math display">\[
\bar{W}_{j}^{i} = \frac{1}{m_{j}^{i}}\sum_{k=1}^{m_{j}^{i}} W_{kj}^{i}
\]</span></p>
<p>Notice that the number of observations for each system <span class="math inline">\(i\)</span>, <span class="math inline">\(m_{j}^{i}\)</span> for each replication <span class="math inline">\(j\)</span> can vary by the system being considered and the number of observations within each replication <span class="math inline">\(j\)</span> can be different. Thus, the differences are <span class="math inline">\(D_j = \bar{W}_{j}^{1} - \bar{W}_{j}^{2}\)</span> for <span class="math inline">\(j = 1, 2, \cdots, r\)</span> and <span class="math inline">\(\bar{D} = \hat{\theta}_1 - \hat{\theta}_2\)</span> is our estimator of the difference between system 1 and system 2.</p>
<p>For CRN to work, we need to induce a positive correlation within the pairs <span class="math inline">\((\bar{W}_{j}^{1}, \bar{W}_{j}^{2})\)</span> for <span class="math inline">\(j = 1, 2, \cdots, r\)</span>. How can we possibly do that using common random numbers? Note that we are not <strong>directly</strong> generating <span class="math inline">\(\bar{W}_{j}^{i}\)</span> using an inverse cumulative distribution function as in the previously discussed simple case of <span class="math inline">\(D_i = X_i - Y_i\)</span>. The random variables <span class="math inline">\(\bar{W}_{j}^{i}\)</span> are obtained from averaging the observed waiting times, <span class="math inline">\(W_{kj}^{i}\)</span>, for system <span class="math inline">\(i\)</span> for the <span class="math inline">\(k = 1, 2, \cdots,m_{j}^{i}\)</span> customers in replication <span class="math inline">\(j\)</span>. Also, note that we are not <strong>directly</strong> generating (via inverse transform functions) the waiting time observations for each customer, <span class="math inline">\(W_{kj}^{i}\)</span>.</p>
<p>Focusing on a single replication with subscript <span class="math inline">\(j\)</span>, lets consider how the random numbers are used within the replication. Let <span class="math inline">\(A_{kj}\)</span> be the <span class="math inline">\(k^{th}\)</span> inter-arrival time, where <span class="math inline">\(a_{1j}\)</span> is the realized inter-arrival time of the first customer, and so forth on the <span class="math inline">\(j^{th}\)</span> replication. Let <span class="math inline">\(S_{kj}\)</span> be the service time of the <span class="math inline">\(k^{th}\)</span> customer, where <span class="math inline">\(s_{1j}\)</span> is the realized service time of the first customer, and so forth on the <span class="math inline">\(j^{th}\)</span> replication. Assume that we use the inverse transform technique for generating <span class="math inline">\(A_{kj}\)</span> and <span class="math inline">\(S_{kj}\)</span>, such that the <span class="math inline">\(k^{th}\)</span> customers inter-arrival and service times are governed by <span class="math inline">\(a_{kj} = F_{A}^{-1}(u_{kj})\)</span> and <span class="math inline">\(s_{kj} = F_S^{-1}(v_{kj})\)</span>, where <span class="math inline">\(U_{kj} \sim U(0,1)\)</span> and <span class="math inline">\(V_{kj} \sim U(0,1)\)</span> resulting in realizations, <span class="math inline">\(u_{kj}\)</span> and <span class="math inline">\(v_{kj}\)</span>. Note that in what follows I have denoted <span class="math inline">\(m_{j}^{1}\)</span> with subscript <span class="math inline">\(j\)</span> to denote the<span class="math inline">\(j^{th}\)</span> replication and with superscript <span class="math inline">\(1\)</span> to denote system 1.</p>
<p><span class="math display">\[
u_{1j}, u_{2j}, u_{3j}, \cdots, u_{m_{j}^{1}j} \\
a_{1j}, a_{2j}, a_{3j}, \cdots, a_{m_{j}^{1}j} \\
v_{1j}, v_{2j}, v_{3j}, \cdots, v_{m_{j}^{1}j} \\
s_{1j}, s_{2j}, s_{3j}, \cdots, s_{m_{j}^{1}j} \\
\vdots \\
\downarrow \\
w_{1j}, w_{2j}, w_{3j}, \cdots, w_{m_{j}^{1}j} \rightarrow \bar{w}_{j}^{1}
\]</span>
Given values for <span class="math inline">\(a_{kj}\)</span> and <span class="math inline">\(s_{kj}\)</span>, we can execute the event logic and produce observations of <span class="math inline">\(w_{kj}^{i}\)</span> of the waiting times for the <span class="math inline">\(m_{j}^{i}\)</span> customers within each replication <span class="math inline">\(j\)</span> for system <span class="math inline">\(1\)</span>. Notice that the pseudo random numbers for the first customer, <span class="math inline">\((u_{1j}, v_{1j})\)</span> are used to produced the inter-arrival time and service time for the first customer, <span class="math inline">\((a_{1j} = F_{A^{1}}^{-1}(u_{1j}), s_{1j} = F_{S^{1}}^{-1}(v_{1j}))\)</span>, which then produces the waiting time for the first customer, <span class="math inline">\(w_{1j}\)</span>. That is, <span class="math inline">\((u_{1j}, v_{1j}) \rightarrow (a_{1j}, s_{1j}) \rightarrow w_{1j}\)</span>. Here <span class="math inline">\(A^{1}\)</span> and <span class="math inline">\(S^{1}\)</span> denote the arrival and service time distributions for system <span class="math inline">\(1\)</span>.</p>
<p>Now, consider the simulation of system 2.</p>
<p><span class="math display">\[
u_{1j}, u_{2j}, u_{3j}, \cdots, u_{m_{j}^{2}j} \\
a_{1j}, a_{2j}, a_{3j}, \cdots, a_{m_{j}^{2}j} \\
v_{1j}, v_{2j}, v_{3j}, \cdots, v_{m_{j}^{2}j} \\
s_{1j}, s_{2j}, s_{3j}, \cdots, s_{m_{j}^{2}j} \\
\vdots \\
\downarrow \\
w_{1j}, w_{2j}, w_{3j}, \cdots, w_{m_{j}^{2}j} \rightarrow \bar{w}_{j}^{2}
\]</span>
When using common random numbers the same sequences of pseudo-random numbers <span class="math inline">\((u_{1j}, u_{2j}, u_{3j}, \cdots, u_{m_{j}^{i}j})\)</span> and <span class="math inline">\((v_{1j}, v_{2j}, v_{3j}, \cdots, v_{m_{j}^{i}j})\)</span> will be used for each replication of system <span class="math inline">\(i\)</span>. This will <strong>not</strong> imply that the same inter-arrival and service time values, <span class="math inline">\((a_{1j}, s_{1j})\)</span>, will be used in the two experiments because inter-arrival distribution for system 1, <span class="math inline">\(F_{A^1}\)</span>, is not the same as the inter-arrival distribution for system 2, <span class="math inline">\(F_{A^2}\)</span>, because the arrival rate of system 1 <span class="math inline">\((\lambda_1)\)</span> is bigger than the arrival rate of system 2, <span class="math inline">\((\lambda_2)\)</span>. However, it should be clear that the inter-arrival times for the two systems will be correlated because a large <span class="math inline">\(u_{1j}\)</span> will produce a large <span class="math inline">\(a^{1}_{1j}\)</span> for system 1 and a large <span class="math inline">\(a^{2}_{1j}\)</span> for system 2. In addition, because the same <span class="math inline">\((v_{1j}, v_{2j}, v_{3j}, \cdots, v_{m_{j}^{i}j})\)</span> will be used and the service time distributions are the same for system 1 and system 2, the same service times will be used for each customer. That is, each customer will experience exactly the same service time when executing the experiments for system 1 and system 2. These insights should suggest to you that correlation will be induced between pairs <span class="math inline">\((\bar{W}_{j}^{1}, \bar{W}_{j}^{2})\)</span> and thus a variance reduction is likely. Note that system 1 will use more pseudo random numbers <span class="math inline">\((u_{1j}, u_{2j}, u_{3j}, \cdots)\)</span> than system 2 because system ones arrival rate is higher than system twos arrival rate, thereby, causing <span class="math inline">\(m_{j}^{1} &gt; m_{j}^{2}\)</span> for <span class="math inline">\(j=1, 2, \cdots, r\)</span>.</p>
<p>A best practice for inducing correlation is to try to <em>synchronize</em> the use of the pseudo-random numbers when executing the simulation experiments. Best practices for synchronization include:</p>
<ul>
<li><p>Utilize the inverse transform technique to generate random variates. The KSL does this by default. Methods such as acceptance/rejection do not have a one-to-one mapping between <span class="math inline">\(U_i\)</span> and <span class="math inline">\(X_i\)</span>. A one-to-one mapping occurs by default for the inverse transform technique.</p></li>
<li><p>Dedicate a different stream to each underlying source of randomness within the model. The KSL does this automatically.</p></li>
<li><p>Dedicate all the random variates to be used by an entity when the entity is created and store them in attributes for when they are needed. This may be impractical due to storage/memory issues or it may be impossible to know in advance if the attribute will be needed. In addition, in general, it takes extra effort to implement this approach in most simulation packages.</p></li>
<li><p>If unable to fully synchronize, then you should try to ensure that the random variables that cannot be synchronized are independent (i.e.use different streams).</p></li>
<li><p>Make sure that each replication (sample path) starts in the same place for each synchronized random number stream. The KSL does this automatically by advancing to the next sub-stream of the current stream for each random variable at the beginning of each replication. This ensures that at the beginning of each replication, the same pseudo-random numbers are used.</p></li>
</ul>
<p>CRN should work if the performance measure of interest for the alternatives is monotonic (in the same direction for the systems) for the random number stream. A large value of the random variable that causes <span class="math inline">\(\hat{\theta}_{1}\)</span> to go up will also cause <span class="math inline">\(\hat{\theta}_{2}\)</span> to go up. Or, a large value of the random variable that causes <span class="math inline">\(\hat{\theta}_{1}\)</span> to go down will also cause <span class="math inline">\(\hat{\theta}_{2}\)</span> to go down. In addition, a small value of the random variable causes <span class="math inline">\(\hat{\theta}_{1}\)</span> to go up will also cause <span class="math inline">\(\hat{\theta}_{2}\)</span> to go up. Or, a small value of the random variable that causes <span class="math inline">\(\hat{\theta}_{1}\)</span> to go down will also cause <span class="math inline">\(\hat{\theta}_{2}\)</span> to go down. These rules are about inducing positive correlation. If negative correlation occurs, then the variance will be inflated. The good news is that you can always check for negative correlation and not utilize CRN if negative correlation is being observed. Because you are likely to run some small pilot runs to plan the experiments (e.g.sample size), I recommend using those pilot runs to also check if CRN is working as intended. Almost all modern simulation languages facilitate simulation using common random numbers by allowing the user to specify streams.</p>
</div>
<div id="antithetic-variates-av" class="section level3 hasAnchor" number="9.2.2">
<h3><span class="header-section-number">9.2.2</span> Antithetic Variates (AV)<a href="ch9VRTs.html#antithetic-variates-av" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Antithetic variates is a class of variance reduction technique that utilizes the manipulation of the underlying random number streams to change the sampling distribution. In contrast to CRN, antithetic variates is applicable to the reduction of variance for estimators from a single simulation experiment (single system). In antithetic variates, we attempt to induce correlation between the (executions or runs) of the simulation. Suppose that we can generate observations of our performance measure <span class="math inline">\((Y_1, Y_2, \cdots, Y_n)\)</span> where <span class="math inline">\(Y_i\)</span> is the observed performance of on run <span class="math inline">\(i\)</span> of the simulation. Here the observations <span class="math inline">\((Y_1, Y_2, \cdots, Y_n)\)</span> are not assumed to be independent. Thus, we call them <em>runs</em> rather than <em>replications</em>.</p>
<p>Assume that we have a covariance stationary process with <span class="math inline">\((Y_1, Y_2, \cdots, Y_n)\)</span> with <span class="math inline">\(E[Y_i]=\theta\)</span> and <span class="math inline">\(var[Y_i] = \sigma^{2}_{Y}\)</span>. We can derive the variance of <span class="math inline">\(\bar{Y}\)</span> as follows:</p>
<p><span class="math display">\[
\text{Var}\left[\bar{Y}\right] = \dfrac{\sigma^2_Y}{n} \Bigg[1 + 2\sum_{j=1}^{n-1} \Bigg(1 - \dfrac{j}{n}\Bigg) \rho_j \Bigg]
\]</span></p>
<p>where <span class="math display">\[\rho_j = \frac{cov(Y_i, Y_{i+j})}{\sigma^2_Y}\]</span></p>
<p>Thus, if we can induce negative correlation, then the <span class="math inline">\(\text{Var}\left[\bar{Y}\right]\)</span> can be reduced. The trick is to try to induce negative correlation and still be able to form valid confidence intervals. To do this we induce correlation within pairs <span class="math inline">\((Y_i, Y_{i+1})\)</span> by manipulating the random number streams.</p>
<p>Let <span class="math inline">\(n\)</span> be the sample size and ensure that <span class="math inline">\(n\)</span> is even. Consider the sequence of pairs, <span class="math inline">\((Y_{2j-1}, Y_{2j})\)</span> for <span class="math inline">\(j=1,2,\cdots, (n/2)\)</span> where the random variables within a pair are dependent but they are independent across the pairs. With <span class="math inline">\(m = n/2\)</span>, define the AV estimator as:</p>
<p><span class="math display">\[
\hat{Y} = \frac{1}{m}\sum_{j=1}^{m}\frac{Y_{2j-1} + Y_{2j}}{2}=\frac{1}{m}\sum_{j=1}^{m}\hat{Y_j}= \bar{Y}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\hat{Y_j} =\frac{Y_{2j-1} + Y_{2j}}{2}
\]</span></p>
<p>The variance of <span class="math inline">\(\hat{Y}\)</span> is reduced by inducing a negative correlation within pairs <span class="math inline">\((Y_i, Y_{i+1})\)</span>. Confidence intervals can be computed by treating the sequence <span class="math inline">\((\hat{Y_1}, \hat{Y_2}, \cdots, \hat{Y_m})\)</span> as a random sample of size <span class="math inline">\(m\)</span> and applying confidence interval methods.</p>
<p>Lets take a look at how AV works. Consider the <span class="math inline">\(var[\hat{Y}]\)</span>
<span class="math display">\[
\text{Var}[\hat{Y}] = \frac{\text{Var}(\hat{Y_j})}{m}=\frac{1}{m}\Bigg[ \text{Var}\Bigg( \frac{Y_{2j-1} + Y_{2j}}{2}\Bigg) \Bigg]=\frac{1}{4m}\Bigg[ \text{Var}[Y_{2j-1}]+ \text{Var}[Y_{2j}]+2cov[Y_{2j-1},Y_{2j}]\Bigg]
\]</span></p>
<p>Since the process is covariance stationary <span class="math inline">\(\text{Var}[Y_{2j-1}] = \text{Var}[Y_{2j}] = \sigma^2_Y\)</span>. Thus,</p>
<p><span class="math display">\[
\text{Var}[\hat{Y_j}] =\frac{1}{4}\Bigg[ \sigma^2_Y + \sigma^2_Y+2\sigma^2_Y \rho\Bigg]=\frac{\sigma^2_Y}{2}(1+\rho)
\]</span>
where the correlation is defined as:</p>
<p><span class="math display">\[
\rho = corr(Y_{2j-1}, Y_{2j}) = \frac{cov(Y_{2j-1}, Y_{2j})}{\sigma^2_Y}
\]</span></p>
<p>Because</p>
<p><span class="math display">\[
\text{Var}[\hat{Y_j}] =\frac{1}{4}\Bigg[ \sigma^2_Y + \sigma^2_Y+2\sigma^2_Y \rho\Bigg]=\frac{\sigma^2_Y}{2}(1+\rho)
\]</span></p>
<p>we have</p>
<p><span class="math display">\[
\text{Var}[\hat{Y}]=\frac{\sigma^2_Y}{2m}(1+\rho)=\frac{\sigma^2_Y}{n}(1+\rho)
\]</span></p>
<p>Therefore, if <span class="math inline">\(\rho &lt; 0\)</span>, then <span class="math inline">\(\text{Var}[\hat{Y}] &lt; \frac{\sigma^2_Y}{n}\)</span>. This implies a variance reduction that is directly proportional to the amount of negative correlation that can be induced within the pairs.</p>
<p>To implement antithetic variates, we can consider the following recipe:</p>
<ul>
<li>Each simulation run (like a replication) use random number streams such that alternating runs use antithetic streams.
<ul>
<li>Run 1: <span class="math inline">\((u_1, u_2, \cdots, u_k)\)</span></li>
<li>Run 2: <span class="math inline">\((1-u_1, 1-u_2, \cdots, 1-u_k)\)</span></li>
<li>Run 3: <span class="math inline">\((u_{k+1}, u_{k+2}, \cdots, u_{2k})\)</span></li>
<li>Run 4: <span class="math inline">\((1-u_{k+1}, 1-u_{k+2}, \cdots, 1-u_{2k})\)</span>, etc.</li>
</ul></li>
<li>Each stream is mapped to some distribution, <span class="math inline">\(F(x)\)</span> in the model, then, suppose,<span class="math inline">\(X_{2j-1}\)</span> and <span class="math inline">\(X_{2j}\)</span> are inputs on runs <span class="math inline">\(2j-1\)</span> and <span class="math inline">\(2j\)</span>, where <span class="math inline">\(j=1,2,\cdots,m\)</span>. Then, if the inverse transform method is used we have <span class="math inline">\(X_{2j-1}=F^{-1}(u)\)</span> and <span class="math inline">\(X_{2j}=F^{-1}(1-u)\)</span>.</li>
</ul>
<p>If the underlying random variables map to outputs monotonically, similar to the CRN situation, then the negative correlation may be preserved within the pairs for the <em>simulation</em> performance measures, which will cause a variance reduction. Just like in CRN, synchronization is important and the same best practices are recommended.</p>
<p>An example of how to perform an antithetic experiment within a Monte Carlo context was presented in <a href="ch2generator.html#ch2antitheticStreams">2.1.4</a>. Within a simple Monte Carlo context the implementation of antithetic variates is relatively straight-forward. Reviewing the implementation of the class <code>MC1DIntegration</code> within the <code>ksl.utilities.mcintegration</code> package is illustrative of the basic approach. The <code>MC1DIntegration</code> class provides for the use of antithetic variates when estimating the area of a 1-dimensional function. In the following Kotlin code, we see that in the <code>init</code> block if the <code>antitheticOption</code> is true an antithetic instance is created from the random variable used in the sampling.</p>
<div class="sourceCode" id="cb372"><pre class="sourceCode kotlin"><code class="sourceCode kotlin"><span id="cb372-1"><a href="ch9VRTs.html#cb372-1" tabindex="-1"></a><span class="kw">class</span> MC1DIntegration <span class="op">(</span></span>
<span id="cb372-2"><a href="ch9VRTs.html#cb372-2" tabindex="-1"></a>    <span class="va">function</span><span class="op">:</span> <span class="dt">FunctionIfc</span><span class="op">,</span></span>
<span id="cb372-3"><a href="ch9VRTs.html#cb372-3" tabindex="-1"></a>    <span class="va">sampler</span><span class="op">:</span> <span class="dt">RVariableIfc</span><span class="op">,</span></span>
<span id="cb372-4"><a href="ch9VRTs.html#cb372-4" tabindex="-1"></a>    <span class="va">antitheticOption</span><span class="op">:</span> <span class="dt">Boolean</span> <span class="op">=</span> true</span>
<span id="cb372-5"><a href="ch9VRTs.html#cb372-5" tabindex="-1"></a><span class="op">)</span> <span class="op">:</span> <span class="dt">MCExperiment</span><span class="op">()</span> <span class="op">{</span></span>
<span id="cb372-6"><a href="ch9VRTs.html#cb372-6" tabindex="-1"></a>    <span class="kw">protected</span> <span class="kw">val</span> <span class="va">myFunction</span><span class="op">:</span> FunctionIfc</span>
<span id="cb372-7"><a href="ch9VRTs.html#cb372-7" tabindex="-1"></a>    <span class="kw">protected</span> <span class="kw">val</span> <span class="va">mySampler</span><span class="op">:</span> RVariableIfc</span>
<span id="cb372-8"><a href="ch9VRTs.html#cb372-8" tabindex="-1"></a>    <span class="kw">protected</span> <span class="kw">var</span> <span class="va">myAntitheticSampler</span><span class="op">:</span> RVariableIfc<span class="op">?</span> <span class="op">=</span> <span class="kw">null</span></span>
<span id="cb372-9"><a href="ch9VRTs.html#cb372-9" tabindex="-1"></a></span>
<span id="cb372-10"><a href="ch9VRTs.html#cb372-10" tabindex="-1"></a>    init <span class="op">{</span></span>
<span id="cb372-11"><a href="ch9VRTs.html#cb372-11" tabindex="-1"></a>        myFunction <span class="op">=</span> function</span>
<span id="cb372-12"><a href="ch9VRTs.html#cb372-12" tabindex="-1"></a>        mySampler <span class="op">=</span> sampler</span>
<span id="cb372-13"><a href="ch9VRTs.html#cb372-13" tabindex="-1"></a>        <span class="cf">if</span> <span class="op">(</span>antitheticOption<span class="op">){</span></span>
<span id="cb372-14"><a href="ch9VRTs.html#cb372-14" tabindex="-1"></a>            myAntitheticSampler <span class="op">=</span> sampler<span class="op">.</span>antitheticInstance<span class="op">()</span></span>
<span id="cb372-15"><a href="ch9VRTs.html#cb372-15" tabindex="-1"></a>        <span class="op">}</span></span>
<span id="cb372-16"><a href="ch9VRTs.html#cb372-16" tabindex="-1"></a>        confidenceLevel <span class="op">=</span> <span class="fl">0.99</span></span>
<span id="cb372-17"><a href="ch9VRTs.html#cb372-17" tabindex="-1"></a>    <span class="op">}</span></span></code></pre></div>
<p>Recall that the <code>MCExperiment</code> class, see Section <a href="mcmExperiments.html#mcmExperiments">3.8</a>, requires the user to provide an implementation of the <code>MCReplicationIfc</code> interface. In the <code>MC1DIntegration</code> class we have the following implementation.</p>
<div class="sourceCode" id="cb373"><pre class="sourceCode kotlin"><code class="sourceCode kotlin"><span id="cb373-1"><a href="ch9VRTs.html#cb373-1" tabindex="-1"></a>    <span class="kw">override</span> <span class="kw">fun</span> <span class="fu">replication</span><span class="op">(</span><span class="va">j</span><span class="op">:</span> <span class="dt">Int</span><span class="op">):</span> <span class="dt">Double</span> <span class="op">{</span></span>
<span id="cb373-2"><a href="ch9VRTs.html#cb373-2" tabindex="-1"></a>        <span class="kw">return</span> <span class="cf">if</span> <span class="op">(</span>isAntitheticOptionOn<span class="op">)</span> <span class="op">{</span></span>
<span id="cb373-3"><a href="ch9VRTs.html#cb373-3" tabindex="-1"></a>            <span class="kw">val</span> <span class="va">y1</span> <span class="op">=</span> myFunction<span class="op">.</span>f<span class="op">(</span>mySampler<span class="op">.</span>sample<span class="op">())</span></span>
<span id="cb373-4"><a href="ch9VRTs.html#cb373-4" tabindex="-1"></a>            <span class="kw">val</span> <span class="va">y2</span> <span class="op">=</span> myFunction<span class="op">.</span>f<span class="op">(</span>myAntitheticSampler<span class="op">!!.</span>sample<span class="op">())</span></span>
<span id="cb373-5"><a href="ch9VRTs.html#cb373-5" tabindex="-1"></a>            <span class="op">(</span>y1 <span class="op">+</span> y2<span class="op">)</span> <span class="op">/</span> <span class="fl">2.0</span></span>
<span id="cb373-6"><a href="ch9VRTs.html#cb373-6" tabindex="-1"></a>        <span class="op">}</span> <span class="cf">else</span> <span class="op">{</span></span>
<span id="cb373-7"><a href="ch9VRTs.html#cb373-7" tabindex="-1"></a>            myFunction<span class="op">.</span>f<span class="op">(</span>mySampler<span class="op">.</span>sample<span class="op">())</span></span>
<span id="cb373-8"><a href="ch9VRTs.html#cb373-8" tabindex="-1"></a>        <span class="op">}</span></span>
<span id="cb373-9"><a href="ch9VRTs.html#cb373-9" tabindex="-1"></a>    <span class="op">}</span></span></code></pre></div>
<p>Note that if the antithetic variate option is turned on, then the <code>replication(j: Int)</code> function returns</p>
<p><span class="math display">\[
\hat{Y_j} =\frac{Y_{2j-1} + Y_{2j}}{2}
\]</span></p>
<p>by using the antithetic sampler for <span class="math inline">\(Y_{2j}\)</span>. Thus, each replication is defined as a pair of runs (antithetic off, antithetic on). This implementation uses a separate antithetic sampler; however, as shown in Section <a href="ch2generator.html#ch2antitheticStreams">2.1.4</a> the same effect can be achieved by using one stream, resetting it and turning on the antithetic option for the stream.</p>
<p>Implementing antithetic variates within the context of DEDS is more complicated. Similar to the discussion associated with CRN, lets consider what happens for the pairs of runs <span class="math inline">\(2j-1\)</span> and <span class="math inline">\(2j\)</span>.</p>
<p>For the <span class="math inline">\(2j-1\)</span> run we have:
<span class="math display">\[
u_{1}, u_{2}, u_{3}, \cdots, u_{m} \\
a_{1}, a_{2}, a_{3}, \cdots, a_{m} \\
v_{1}, v_{2}, v_{3}, \cdots, v_{m} \\
s_{1}, s_{2}, s_{3}, \cdots, s_{m} \\
\vdots \\
\downarrow \\
w_{1}, w_{2}, w_{3}, \cdots, w_{m} \rightarrow \bar{w}_{2j-1}
\]</span>
For the <span class="math inline">\(2j\)</span> run we use the antithetic variates such that, <span class="math inline">\(a_{k} = F_{A}^{-1}(1-u_{k})\)</span> and <span class="math inline">\(s_{k} = F_S^{-1}(1-v_{k})\)</span>:
<span class="math display">\[
1-u_{1}, 1-u_{2}, 1-u_{3}, \cdots, 1-u_{m} \\
a_{1}, a_{2}, a_{3}, \cdots, a_{m} \\
1-v_{1}, 1-v_{2}, 1-v_{3}, \cdots, 1-v_{m} \\
s_{1}, s_{2}, s_{3}, \cdots, s_{m} \\
\vdots \\
\downarrow \\
w_{1}, w_{2}, w_{3}, \cdots, w_{m} \rightarrow \bar{w}_{2j}
\]</span>
This should cause the pairs <span class="math inline">\((\bar{W}_{2j-1}, \bar{W}_{2j})\)</span> to have negative correlation. Again the actual realization of some variance reduction will depend upon synchronization strategies and responses, such as the waiting time, responding in a monotonic fashion to the antithetic sampling.</p>
<p>Within a DEDS context, the KSL permits the model to be executed and the streams controlled to implement antithetic sampling. The property <code>antitheticOption</code> of the <code>Model</code> class controls whether the streams use antithetic sampling. It may be instructive to see how this is implemented within the <code>Model</code> class.</p>
<div class="sourceCode" id="cb374"><pre class="sourceCode kotlin"><code class="sourceCode kotlin"><span id="cb374-1"><a href="ch9VRTs.html#cb374-1" tabindex="-1"></a>    <span class="kw">private</span> <span class="kw">fun</span> <span class="fu">handleAntitheticReplications</span><span class="op">()</span> <span class="op">{</span></span>
<span id="cb374-2"><a href="ch9VRTs.html#cb374-2" tabindex="-1"></a>        <span class="co">// handle antithetic replications</span></span>
<span id="cb374-3"><a href="ch9VRTs.html#cb374-3" tabindex="-1"></a>        <span class="cf">if</span> <span class="op">(</span>antitheticOption<span class="op">)</span> <span class="op">{</span></span>
<span id="cb374-4"><a href="ch9VRTs.html#cb374-4" tabindex="-1"></a>            logger<span class="op">.</span>info <span class="op">{</span> <span class="st">&quot;Executing handleAntitheticReplications() setup&quot;</span> <span class="op">}</span></span>
<span id="cb374-5"><a href="ch9VRTs.html#cb374-5" tabindex="-1"></a>            <span class="cf">if</span> <span class="op">(</span>currentReplicationNumber <span class="op">%</span> <span class="dv">2</span> <span class="op">==</span> <span class="dv">0</span><span class="op">)</span> <span class="op">{</span></span>
<span id="cb374-6"><a href="ch9VRTs.html#cb374-6" tabindex="-1"></a>                <span class="co">// even number replication</span></span>
<span id="cb374-7"><a href="ch9VRTs.html#cb374-7" tabindex="-1"></a>                <span class="co">// return to beginning of sub-stream</span></span>
<span id="cb374-8"><a href="ch9VRTs.html#cb374-8" tabindex="-1"></a>                resetStartSubStream<span class="op">()</span></span>
<span id="cb374-9"><a href="ch9VRTs.html#cb374-9" tabindex="-1"></a>                <span class="co">// turn on antithetic sampling</span></span>
<span id="cb374-10"><a href="ch9VRTs.html#cb374-10" tabindex="-1"></a>                antitheticOption<span class="op">(</span><span class="kw">true</span><span class="op">)</span></span>
<span id="cb374-11"><a href="ch9VRTs.html#cb374-11" tabindex="-1"></a>            <span class="op">}</span> <span class="cf">else</span>  <span class="co">// odd number replication</span></span>
<span id="cb374-12"><a href="ch9VRTs.html#cb374-12" tabindex="-1"></a>                <span class="cf">if</span> <span class="op">(</span>currentReplicationNumber <span class="op">&gt;</span> <span class="dv">1</span><span class="op">)</span> <span class="op">{</span></span>
<span id="cb374-13"><a href="ch9VRTs.html#cb374-13" tabindex="-1"></a>                    <span class="co">// turn off antithetic sampling</span></span>
<span id="cb374-14"><a href="ch9VRTs.html#cb374-14" tabindex="-1"></a>                    antitheticOption<span class="op">(</span><span class="kw">false</span><span class="op">)</span></span>
<span id="cb374-15"><a href="ch9VRTs.html#cb374-15" tabindex="-1"></a>                    <span class="co">// advance to next sub-stream</span></span>
<span id="cb374-16"><a href="ch9VRTs.html#cb374-16" tabindex="-1"></a>                    advanceToNextSubStream<span class="op">()</span></span>
<span id="cb374-17"><a href="ch9VRTs.html#cb374-17" tabindex="-1"></a>                <span class="op">}</span></span>
<span id="cb374-18"><a href="ch9VRTs.html#cb374-18" tabindex="-1"></a>        <span class="op">}</span></span>
<span id="cb374-19"><a href="ch9VRTs.html#cb374-19" tabindex="-1"></a>    <span class="op">}</span></span></code></pre></div>
<p>In the previous code, we see that if the replication number is even (a multiple of 2), the the streams are reset to the beginning of their current sub-stream and the antithetic option is turned on for <em>every</em> random variable (instance of the <code>RandomVariable</code> class) used within the model. This causes the sampling performed during an even run to use antithetic pseudo-random numbers. Then, for odd replications, the antithetic option is turned off and the normal stream advancement occurs. Thus, within the KSL a simple change of a property permits antithetic sampling.</p>
</div>
<div id="indirect-estimation" class="section level3 hasAnchor" number="9.2.3">
<h3><span class="header-section-number">9.2.3</span> Indirect Estimation<a href="ch9VRTs.html#indirect-estimation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>While not as generalizable as CRN or AV, indirect estimation is interesting because it illustrates the general concept of replacing uncertainty with knowledge to reduce variance. The basic idea is to exploit an analytical relationship involving the parameter of interest with other random variables whose population values are known. The most useful application of indirect estimation has been the use of Littles formula or other conservation law relationships for simulations involving queueuing.</p>
<p>Suppose we are interested in estimating <span class="math inline">\(W_q\)</span>, <span class="math inline">\(W_s\)</span>, <span class="math inline">\(L_q\)</span>, and <span class="math inline">\(L_s\)</span> in a GI/G/c queueing simulation. Littles formulat tells us that <span class="math inline">\(L_s = \lambda W_s\)</span>,<span class="math inline">\(L_q = \lambda W_q\)</span>, and <span class="math inline">\(B = L_s - L_q = \lambda E[ST]\)</span>, where <span class="math inline">\(E[ST]\)</span> is the mean of the service time distribution and <span class="math inline">\(\lambda\)</span> is the mean arrival rate. We also know that <span class="math inline">\(W_s = W_q + E[ST]\)</span>. During the simulation, we can directly observe the waiting time <span class="math inline">\(W_{q_i}\)</span> of each customer in the queue and estimate <span class="math inline">\(W_q\)</span> with:</p>
<p><span class="math display">\[
\hat{W}_q = \frac{1}{n}\sum_{i=1}^{n}W_{q_i}
\]</span></p>
<p>Indirect estimation suggests that we estimate the other performance measures using the operational relationships.</p>
<ul>
<li><span class="math inline">\(\hat{W}_s = \hat{W}_q + E[ST]\)</span></li>
<li><span class="math inline">\(\hat{L}_q = \lambda \hat{W}_q\)</span>, and</li>
<li><span class="math inline">\(\hat{L}_s = \lambda (\hat{W}_q + E[ST])\)</span></li>
</ul>
<p>Notice that known quantities <span class="math inline">\(E[ST]\)</span> and <span class="math inline">\(\lambda\)</span> are used. This replaces variability with certainty causing a variance reduction. Experimental studies have shown that this approach will work well provided that the operational formulas are applicable. However, this approach requires the implementation of the analytical relationships and formulation of the estimators which is not standard in any simulation packages. For further references the interested reader should consult Chapter 11 of <span class="citation">(<a href="#ref-law2007simulation">Law 2007</a>)</span>.</p>
</div>
<div id="control-variates-cv" class="section level3 hasAnchor" number="9.2.4">
<h3><span class="header-section-number">9.2.4</span> Control Variates (CV)<a href="ch9VRTs.html#control-variates-cv" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The idea is to exploit knowledge of the system by other variables for which the true mean is known and exploit any dependence that it might have with our parameter of interest. Lets consider a simplified example.</p>
<hr />
<div class="example">
<p><span id="exm:CVExample" class="example"><strong>Example 9.5  (Simple Health Clinic) </strong></span>Suppose that we are simulating a heart disease reduction program operating between 10am and Noon. Suppose that 30 patients per day are scheduled at 5 minute intervals. The clinic process is as follows. The patient first visits a clerk to gather their medical information. Then, the patient visits a medical technician where blood pressure and vitals are recorded and it is determined whether or not the patient should visit the nurse. If the patient visits the nurse practitioner, then they receive some medical service and return to a clerk for payment. If the patient does not need to see the nurse practitioner, then the patient goes directly to the clerk for checkout.</p>
<ul>
<li><p>Let <span class="math inline">\(Y_j\)</span> be the utilization of the nurse on the <span class="math inline">\(j^{th}\)</span> day. We are interested in estimating the utilization of the nurse on any given day.</p></li>
<li><p>Let <span class="math inline">\(X_j\)</span> be the number of patients who consulted with the nurse on the <span class="math inline">\(j^{th}\)</span> day.</p></li>
<li><p>Let <span class="math inline">\(p\)</span> be the proportion of patients that see the nurse. Because there are 30 patients per day, we have <span class="math inline">\(E[X_j] = 30p = \mu\)</span>, which is a known quantity.</p></li>
</ul>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:CVFigure"></span>
<img src="figures2/ch9/ControlVariateExample.PNG" alt="Simple Health Clinic" width="90%" height="90%" />
<p class="caption">
Figure 9.4: Simple Health Clinic
</p>
</div>
</div>
<hr />
<p>To build up a control variate estimator, define the following random variable, <span class="math inline">\(Z_j = Y_j + C(X_j - \mu)\)</span>, where <span class="math inline">\(C\)</span> is assumed to be a known constant. Since, our parameter of interest is <span class="math inline">\(E[Y_j]\)</span> consider <span class="math inline">\(E[Z_j]\)</span>
<span class="math display">\[
E[Z_j] = E[Y_j + C(X_j - \mu)] = E[Y_j]+C(E[X_j] -\mu)= E[Y_j]
\]</span>
Thus, we can estimate <span class="math inline">\(E[Y_j]\)</span> by observing <span class="math inline">\(Z_j\)</span>, and computing</p>
<p><span class="math display">\[
\bar{Z}= \frac{1}{n}\sum_{i=1}^{n}Z_i
\]</span></p>
<p>The sample average, <span class="math inline">\(\bar{Z}\)</span>, will be an unbiased estimator of <span class="math inline">\(E[Y_j]\)</span> if <span class="math inline">\(a\)</span> is a constant known in advance. The random variable <span class="math inline">\(Z_j\)</span> forms our control variate estimator and the quantity <span class="math inline">\(X_j\)</span> with known mean <span class="math inline">\(\mu\)</span> is called the control variate. Intuitively, we can see that <span class="math inline">\(X_j\)</span> should be related to the utilization of the nurse. For example, if <span class="math inline">\(X_j\)</span> is high for a particular simulation run, then the nurse should see more patients and thus have a higher utilization. Similarly, if <span class="math inline">\(X_j\)</span> is low for a particular simulation run, then the nurse should see less patients and thus have a lower utilization. Thus, we can conclude that <span class="math inline">\(X_j\)</span> is correlated with our quantity of interest <span class="math inline">\(Y_j\)</span>. The linear form <span class="math inline">\(Z_j = Y_j + C(X_j - \mu)\)</span> adjusts <span class="math inline">\(Y_j\)</span> up or down based on what is observed for <span class="math inline">\(X_j\)</span>. Why would we want to do this? That is why does using a control variate work?</p>
<p>Consider deriving the variance of our control variate estimator, <span class="math inline">\(\text{Var}[Z_j]\)</span>. This results in:</p>
<p><span class="math display">\[
\text{Var}[Z_j]= \text{Var}[Y_j + C(X_j -\mu)] = \text{Var}[Y_j] + C^2\text{Var}[X_j]+2C\,\text{cov}(Y_j, X_j)
\]</span>
Rearranging this equation, we have:</p>
<p><span class="math display">\[
\text{Var}[Z_j]= \text{Var}[Y_j] + C(C\text{Var}[X_j]+2\,\text{cov}[Y_j, X_j])
\]</span>
Thus, there will be a variance reduction, <span class="math inline">\(\text{Var}[Z_j] &lt; \text{Var}[Y_j]\)</span>, if <span class="math inline">\(C(C\text{Var}[X_j]+2\,\text{cov}[Y_j, X_j]) &lt; 0\)</span>. That is, we will get a variance reduction if this condition is true. There are two cases to consider. If the constant <span class="math inline">\(C&gt;0\)</span>, then the value <span class="math inline">\(C(C\text{Var}[X_j]+2\,\text{cov}[Y_j, X_j]) &lt; 0\)</span> if</p>
<p><span class="math display">\[C &lt; \frac{-2\text{cov}(Y_j, X_j)}{\text{Var}(X_j)}\]</span></p>
<p>However, if the constant <span class="math inline">\(C&lt;0\)</span>, then the value <span class="math inline">\(C(C\text{Var}[X_j]+2\,\text{cov}[Y_j, X_j]) &gt; 0\)</span>, if
<span class="math display">\[C &gt; \frac{-2\text{cov}(Y_j, X_j)}{\text{Var}(X_j)}\]</span>
Thus, for a suitably chosen value for <span class="math inline">\(C\)</span>, we can reduce the variance. How do we pick <span class="math inline">\(C\)</span>? Well, it makes sense to pick the value of <span class="math inline">\(C\)</span> that minimizes the variance of the estimator, <span class="math inline">\(\text{Var}[Z_j]\)</span>, and thus pick the value of <span class="math inline">\(C\)</span> that maximizes the variance reduction. To minimize <span class="math inline">\(\text{Var}[Z_j]\)</span> we can take the derivative with respect to <span class="math inline">\(C\)</span>, and set the derivative equal to 0, and solve for the value of <span class="math inline">\(C\)</span>.
<span class="math display">\[
\frac{d \, \text{Var}[Z_j]}{d\,C}=2C\,\text{Var}(X_j)+ 2\text{cov}(Y_j, X_j) = 0
\]</span></p>
<p><span class="math display">\[
C^{*}= \frac{-\text{cov}(Y_j, X_j)}{\text{Var}(X_j)}
\]</span></p>
<p>Substituting <span class="math inline">\(C^{*}\)</span> into <span class="math inline">\(\text{Var}[Z_j]\)</span> yields the following:</p>
<p><span class="math display">\[
\text{Var}[Z_j] = \text{Var}[Y_j](1 - \rho^2_{XY})
\]</span>
where <span class="math inline">\(\rho_{XY}\)</span> is the correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, <span class="math inline">\(\text{corr}(Y_j, X_j)\)</span>. Thus, the variance of our estimator, <span class="math inline">\(\text{Var}[Z_j]\)</span>, depends directly on the correlation between <span class="math inline">\(Y_j\)</span> and <span class="math inline">\(X_j\)</span>, with more correlation (either positive or negative), the more variance reduction.</p>
<p>Unfortunately <span class="math inline">\(C^{*}\)</span> is in general unknown and must be estimated from the data. Looking at the form for <span class="math inline">\(C^{*}\)</span>, we need to estimate the numerator,<span class="math inline">\(\text{cov}(Y_j, X_j)\)</span>, and the denominator, <span class="math inline">\(\text{Var}(X_j)\)</span>. Consider estimating <span class="math inline">\(\text{cov}(Y_j, X_j)\)</span> with the following:</p>
<p><span class="math display">\[
\widehat{cov(Y_j, X_j)}=\frac{\sum_{i=1}^{n}(Y_i - \bar{Y})(X_i - \bar{X})}{n-1}
\]</span>
And, consider estimating <span class="math inline">\(\text{Var}(X_j)\)</span> with the following:</p>
<p><span class="math display">\[
\widehat{\text{Var}(X_j)}=\frac{\sum_{i=1}^{n}(X_i - \bar{X})^2}{n-1}
\]</span></p>
<p>Substituting these estimators into the form for <span class="math inline">\(C^{*}\)</span>, this implies an estimator for <span class="math inline">\(C^{*}\)</span> of:</p>
<p><span class="math display">\[
\hat{C}^{*}= \frac{\sum_{i=1}^{n}(Y_i - \bar{Y})(X_i - \bar{X})}{\sum_{i=1}^{n}(X_i - \bar{X})^2}
\]</span></p>
<p>This formula should remind you of the estimator for the slope of a regression line. From the form for <span class="math inline">\(Z_j\)</span> it should now be readily apparent that it is essentially assuming a linear relationship and that the optimal value of <span class="math inline">\(C\)</span> would be the slope of the line (under the assumption that a linear relationship holds). This leads to a more general basis for the form of control variates.</p>
<p>Suppose we want to estimate <span class="math inline">\(\theta = E[Y]\)</span>, and we can also observe some other random variable, <span class="math inline">\(X\)</span>, whose distribution is known, with mean <span class="math inline">\(\mu = E[X]\)</span>. Assume that <span class="math inline">\(E[Y|X=x] \approx \beta_0 + \beta_1(x-\mu)\)</span>. That is, given we observe <span class="math inline">\(x\)</span>, there is an (approximate) linear relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>. This might be justified by thinking of <span class="math inline">\(g(x) = E[Y|X=x]\)</span> as a function of <span class="math inline">\(x\)</span> and noting that a linear relationship could at least be possible within a small range of <span class="math inline">\(\mu\)</span> by expanding <span class="math inline">\(g(x)\)</span> as a Taylor series about <span class="math inline">\(\mu\)</span>.</p>
<p><span class="math display">\[
g(x) \approx g(\mu) + \frac{d\,g(\mu)}{d\,x}(x- \mu)+\cdots
\]</span></p>
<p>Recall that <span class="math inline">\(E[Y]= E\bigg[E[Y|X]\bigg] = \beta_0 + \beta_1(X-\mu) = \beta_0 = \theta\)</span>. Thus, if we can find an estimator for <span class="math inline">\(\beta_0\)</span> <em>and</em> the linear relationship is true, then we will have an unbiased estimator for <span class="math inline">\(\theta\)</span>. But, this is exactly the same form of the previously derived control variate estimator. In what follows, we will assume that the linear form, <span class="math inline">\(E[Y|X=x] = \beta_0 + \beta_1(x-\mu)\)</span> holds.</p>
<p>Assuming that <span class="math inline">\(E[Y|X=x] = \beta_0 + \beta_1(x-\mu)\)</span> is true, then the control variate estimator for <span class="math inline">\(\theta = E[Y]\)</span> is <span class="math inline">\(\hat{\beta_0}\)</span> where:</p>
<p><span class="math display">\[
\hat{\beta_1} = \frac{\sum_{i=1}^{n}(Y_i - \bar{Y})(X_i - \bar{X})}{\sum_{i=1}^{n}(X_i - \bar{X})^2}
\]</span></p>
<p>And,</p>
<p><span class="math display">\[
\hat{\beta_0}= \bar{Y}-\hat{\beta_1}(\bar{X}-\mu)
\]</span></p>
<p>If the linear form for <span class="math inline">\(E[Y|X=x]\)</span> is true, then it can be shown that <span class="math inline">\(E[\hat{\beta_0}] = \beta_0\)</span> and <span class="math inline">\(E[\hat{\beta_1}] = \beta_1\)</span>. That is, the control variate estimator <span class="math inline">\(\hat{\beta_0}\)</span> will be unbiased, and as we have shown, there will be a variance reduction if there is some correlation between the control variate and the quantity of interest. For the technical details of when control variates work and when they do not, please see the following references <span class="citation">(<a href="#ref-nelson-pei">Nelson and Pei. 2021</a>)</span> and <span class="citation">(<a href="#ref-nelson1990">Nelson 1990</a>)</span>. As noted in those references, if the relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> is bi-variate normal then the linear form for <span class="math inline">\(E[Y|X=x]\)</span> is true. In addition, the results can be extended to more than one control variate and if the relationship between the quantity of interest <span class="math inline">\(Y\)</span> and the controls is multi-variate normal then control variates will again work. Since in many cases the observations of <span class="math inline">\(Y\)</span> and the controls are averages of within replication data, it is likely that the assumption of normality will be approximately true. In what follows, we indicate some results from <span class="citation">(<a href="#ref-nelson1990">Nelson 1990</a>)</span> and refer the interested reader to that paper for the proofs.</p>
<p>First, if <span class="math inline">\((Y, X)\)</span> are bivariate normal then it can be shown that:</p>
<p><span class="math display">\[
\text{Var}[\hat{\beta_0}]= (1-\rho^2)\bigg(\frac{n-2}{n-3}\bigg)\frac{\sigma^2_{Y}}{n}
\]</span></p>
<p>From this, we can see that <span class="math inline">\(\text{Var}[\hat{\beta_0}] \leq \text{Var}[\bar{Y}]\)</span> if and only if <span class="math inline">\(\rho^2 \geq 1/(n-2)\)</span>. Suppose <span class="math inline">\(n=10\)</span>, then <span class="math inline">\(\rho \geq \sqrt{0.1} = 0.01\)</span>. Thus for a small amount of correlation, we can get a variance reduction. These result generalize to more than one control variate. Let <span class="math inline">\(q\)</span> be the number of control variates. We then formulate the control variate estimator from a linear model of the following form:</p>
<p><span class="math display">\[
Y_i = \beta_0 + \beta_1 (X_1  - \mu_1) + \beta_2 (X_2  - \mu_2) + \cdots + \beta_q (X_q - \mu_q) + \epsilon_i
\]</span></p>
<p>If the <span class="math inline">\(Y\)</span> and <span class="math inline">\(X_j\)</span> are multi-variate normally distributed, then</p>
<p><span class="math display">\[
var[\hat{\beta_0}]= (1-R^{2}_{XY})\bigg(\frac{n-2}{n-q-2}\bigg)\frac{\sigma^2_{Y}}{n}
\]</span>
There will be a variance reduction if <span class="math inline">\(R^{2}_{XY} \geq \frac{q}{n-2}\)</span>, where <span class="math inline">\(R_{XY}\)</span> is the multiple correlation of <span class="math inline">\(Y\)</span> on <span class="math inline">\(X\)</span>. The control variate estimator for <span class="math inline">\(\theta = E[Y]\)</span> will be <span class="math inline">\(\hat{\beta_0}\)</span>, which is the intercept term for the regression. Thus, any standard regression program can be used to compute the estimate. Everything works if the linear relationship holds and the <span class="math inline">\(Y\)</span> and <span class="math inline">\(X_j\)</span> are multi-variate normal. If the assumptions do not hold, then the control variate estimator will, in general, be biased. It may have smaller variance, but it may not be unbiased. The recommendation is to use a small number of controls <span class="math inline">\(1\leq q\leq 10\)</span> and batch the replications if necessary to make the data more normal. If <span class="math inline">\(n\)</span> is the number of replications and <span class="math inline">\(n \geq 100\)</span> with <span class="math inline">\(1\leq q\leq 5\)</span>, then batch <span class="math inline">\(n\)</span> into between <span class="math inline">\(30 \leq k\leq 60\)</span> batches.</p>
<p>Control variates is a general variance reduction technique. In general within a DEDS situation, there will be many possible controls because any of the input distributions with their known mean values are candidates for controls. With many possible controls, you should think about which input distributions might have more influence over the estimated performance measure as a method for selecting controls. In addition, there is little extra programming that needs to be implemented other than capturing the output of the <span class="math inline">\(Y\)</span> and the controls <span class="math inline">\(X\)</span> and their mean values. There is little possibility that control variates will backfire (increase the variance); however, you need to be aware of the previously mentioned possibility of bias. Thus, checking for the normality of the responses and controls is essential. As mentioned, batching the output might be useful. In addition, jackknifing the estimator may help in reducing possible bias. However, these fixes required advanced sophistication from the analyst and will likely need to be implemented.</p>
<p>The KSL has support for using control variates within the context of a DEDS simulation via the <code>ControlVariateDataCollector</code> class. The <code>ControlVariateDataCollector</code> class works in a similar fashion as the <code>ReplicationDataCollector</code> class that was discussed in Section <a href="simoafinhorizonex.html#simoaCapture">5.4.1</a>. The following code illustrates its use.</p>
<div class="example">
<p><span id="exm:exControlVar" class="example"><strong>Example 9.6  (Illustrating Control Variates) </strong></span>This example illustrates how to define controls, collect the responses, and compute the control variate estimators using KSL regression constructs.</p>
<div class="sourceCode" id="cb375"><pre class="sourceCode kotlin"><code class="sourceCode kotlin"><span id="cb375-1"><a href="ch9VRTs.html#cb375-1" tabindex="-1"></a>    <span class="kw">val</span> <span class="va">model</span> <span class="op">=</span> Model<span class="op">(</span><span class="st">&quot;CV Example&quot;</span><span class="op">)</span></span>
<span id="cb375-2"><a href="ch9VRTs.html#cb375-2" tabindex="-1"></a>    model<span class="op">.</span>numberOfReplications <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb375-3"><a href="ch9VRTs.html#cb375-3" tabindex="-1"></a>    <span class="kw">val</span> <span class="va">palletWorkCenter</span> <span class="op">=</span> PalletWorkCenter<span class="op">(</span>model<span class="op">)</span></span>
<span id="cb375-4"><a href="ch9VRTs.html#cb375-4" tabindex="-1"></a>    <span class="kw">val</span> <span class="va">cvCollector</span> <span class="op">=</span> ControlVariateDataCollector<span class="op">(</span>model<span class="op">)</span></span>
<span id="cb375-5"><a href="ch9VRTs.html#cb375-5" tabindex="-1"></a>    cvCollector<span class="op">.</span>addResponse<span class="op">(</span>palletWorkCenter<span class="op">.</span>totalProcessingTime<span class="op">,</span> <span class="st">&quot;TotalTime&quot;</span><span class="op">)</span></span>
<span id="cb375-6"><a href="ch9VRTs.html#cb375-6" tabindex="-1"></a>    cvCollector<span class="op">.</span>addControlVariate<span class="op">(</span>palletWorkCenter<span class="op">.</span>processingTimeRV<span class="op">,</span> <span class="op">(</span><span class="fl">8.0</span><span class="op">+</span> <span class="fl">12.0</span><span class="op">+</span> <span class="fl">15.0</span><span class="op">)/</span><span class="fl">3.0</span><span class="op">,</span> <span class="st">&quot;PalletTime&quot;</span><span class="op">)</span></span>
<span id="cb375-7"><a href="ch9VRTs.html#cb375-7" tabindex="-1"></a>    cvCollector<span class="op">.</span>addControlVariate<span class="op">(</span>palletWorkCenter<span class="op">.</span>numPalletsRV<span class="op">,</span> <span class="op">(</span><span class="fl">100.0</span><span class="op">*</span><span class="fl">0.8</span><span class="op">),</span> <span class="st">&quot;NumPallets&quot;</span><span class="op">)</span></span>
<span id="cb375-8"><a href="ch9VRTs.html#cb375-8" tabindex="-1"></a>    model<span class="op">.</span>simulate<span class="op">()</span></span>
<span id="cb375-9"><a href="ch9VRTs.html#cb375-9" tabindex="-1"></a>    model<span class="op">.</span>print<span class="op">()</span></span>
<span id="cb375-10"><a href="ch9VRTs.html#cb375-10" tabindex="-1"></a>    println<span class="op">(</span>cvCollector<span class="op">)</span></span>
<span id="cb375-11"><a href="ch9VRTs.html#cb375-11" tabindex="-1"></a>    <span class="kw">val</span> <span class="va">regressionData</span> <span class="op">=</span> cvCollector<span class="op">.</span>collectedData<span class="op">(</span><span class="st">&quot;TotalTime&quot;</span><span class="op">,</span> <span class="dv">20</span><span class="op">)</span></span>
<span id="cb375-12"><a href="ch9VRTs.html#cb375-12" tabindex="-1"></a>    println<span class="op">(</span>regressionData<span class="op">)</span></span>
<span id="cb375-13"><a href="ch9VRTs.html#cb375-13" tabindex="-1"></a>    println<span class="op">()</span></span>
<span id="cb375-14"><a href="ch9VRTs.html#cb375-14" tabindex="-1"></a>    <span class="kw">val</span> <span class="va">regressionResults</span> <span class="op">=</span> cvCollector<span class="op">.</span>regressionResults<span class="op">(</span>regressionData<span class="op">)</span></span>
<span id="cb375-15"><a href="ch9VRTs.html#cb375-15" tabindex="-1"></a>    println<span class="op">(</span>regressionResults<span class="op">)</span></span></code></pre></div>
</div>
<p>Notice the use of the method, <code>cvCollector.collectedData("TotalTime", 20)</code>. This method prepares the data for a regression analysis. The value 20 tells the method to form 20 batches from the 100 replications into 20 batches of size 5. The following output is the standard half-width report for the 100 replications.</p>
<pre><code>Half-Width Statistical Summary Report - Confidence Level (95.000)% 

Name                            Count         Average      Half-Width 
------------------------------------------------------------------------- 
NumBusyWorkers                  100            1.9154          0.0133 
PalletQ:NumInQ                  100            6.9721          0.5609 
PalletQ:TimeInQ                 100           42.1123          3.3255 
Num Pallets at WC               100            8.8875          0.5674 
System Time                     100           53.7523          3.3254 
Total Processing Time           100          497.7907          6.0995 
P{total time &gt; 480 minutes}     100            0.7300          0.0885 
RandomVariable_4:CVResponse     100           11.6400          0.0299 
RandomVariable_6:CVResponse     100           80.8000          0.7316 
Num Processed                   100           81.8000          0.7316 
----------------------------------------------------------------------- </code></pre>
<p>As you can see in the following output from the <code>ControlVariateDataCollector</code> class, the data is organized in a manner that permits the application of linear regression.</p>
<pre><code>Control Variate Collector
Responses:
response: TotalTime

Controls:
control: PalletTime      mean = 11.666666666666666
control: NumPallets      mean = 80.0

Replication Data Collector
   Total Processing Time RandomVariable_4:CVResponse RandomVariable_6:CVResponse
 0            482.417201                   11.991551                        75.0
 1            461.544205                   11.681418                        74.0
 2            521.417293                   11.701470                        78.0
 3            476.025297                   11.699098                        80.0
 4            534.281150                   11.464543                        86.0
 5            485.735690                   11.475689                        82.0
 6            477.468018                   11.645124                        80.0
 7            482.557886                   11.579375                        79.0
 8            499.628817                   11.765892                        82.0
 9            471.007443                   11.603611                        78.0

Control Variate Data
    TotalTime PalletTime NumPallets
 0 482.417201   0.324884       -5.0
 1 461.544205   0.014752       -6.0
 2 521.417293   0.034803       -2.0
 3 476.025297   0.032431        0.0
 4 534.281150  -0.202123        6.0
 5 485.735690  -0.190977        2.0
 6 477.468018  -0.021543        0.0
 7 482.557886  -0.087292       -1.0
 8 499.628817   0.099225        2.0
 9 471.007443  -0.063056       -2.0</code></pre>
<p>Thus, you can use your favorite linear regression analysis software to analyze the data and compute the control variate estimate. The KSL has support for performing ordinary least squares regression. In the previously provided code notice that the control variate collector is used to get the regression data via the lines:</p>
<pre><code>    val regressionData = cvCollector.collectedData(&quot;TotalTime&quot;, 20)
    println(regressionData)</code></pre>
<p>Then, the collector uses the regression data to estimate the regression coefficients.</p>
<pre><code>    val regressionResults = cvCollector.regressionResults(regressionData)
    println(regressionResults)</code></pre>
<p>Below is a snippet of the results of applying the control variate estimator via the estimated regression. The estimate of the intercept term is the control variate estimate.</p>
<pre><code>Regression Results
-------------------------------------------------------------------------------------
Parameter Estimation Results
    Predictor  parameter parameterSE     TValue P-Values LowerLimit UpperLimit
 0  Intercept 494.631368    2.792054 177.156788 0.000000 488.740649 500.522088
 1 PalletTime  89.977513   44.683070   2.013682 0.060151  -4.295525 184.250550
 2 NumPallets   6.943175    1.339181   5.184644 0.000075   4.117751   9.768599
-------------------------------------------------------------------------------------</code></pre>
<p>Notice that the estimated response is different than the standard estimate (497.7907) versus (494.631368). In addition,
the half-width of the control variate estimator is smaller. Further analysis should be performed to check the normality assumptions and assess the quality of the estimated value. The KSL <code>RegressionResultsIfc</code> interface has the ability to check the normality of the residuals perform additional regression analysis operations. This analysis is left as an exercise for the reader.</p>
</div>
<div id="stratified-and-post-stratified-sampling" class="section level3 hasAnchor" number="9.2.5">
<h3><span class="header-section-number">9.2.5</span> Stratified and Post Stratified Sampling<a href="ch9VRTs.html#stratified-and-post-stratified-sampling" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The idea behind stratified sampling is to take advantage of known properties or characteristics within the underlying population that is being sampled. In stratified sampling, conditional expectation is used to reduce the variance by conditioning on another variable (similar to how control variates use a control variable). In stratified sampling, we sample within the different strata and then form an overall estimate from the estimated value from each strata.</p>
<p>For example, suppose that we were estimating the overall GPA of engineering majors (electrical, mechanical, industrial, computer, chemical, biological, etc.). We know that each major has a different number of students within the college of engineering. If we take a simple random sample from the entire population, we might not observe a particular major just due to the sampling. However, if we knew (or could estimate) the proportion of students within each major, then we could randomly sample from each major (strata) and form a combined GPA estimate from the weighted average of the strata averages. This is the basic idea of stratified sampling. We can control the variability of the estimator by how many students we sample from each major.</p>
<p>Lets develop some notation to specify these concepts. Recall that <span class="math inline">\(\theta = E[Y]= E\bigg[E[Y|X]\bigg] = \int E[Y|X=x]d\,F(x)\)</span> where <span class="math inline">\(F(x)\)</span> is the CDF of some random variable <span class="math inline">\(X\)</span>. Let the range of <span class="math inline">\(X\)</span> be denoted by <span class="math inline">\(\Omega\)</span>. Divide <span class="math inline">\(\Omega\)</span> into <span class="math inline">\(k\)</span> sub-ranges <span class="math inline">\(L_i\)</span>, <span class="math inline">\(i=1,2,\cdots,k\)</span>, such that the <span class="math inline">\(L_i\)</span> are mutually exclusive and collectively exhaustive. The <span class="math inline">\(L_i\)</span> are called the strata for <span class="math inline">\(X\)</span>. Using conditional expectation, we have that:</p>
<p><span class="math display">\[
\theta = \sum_{i=1}^kE[Y|x \in L_i]P(X \in L_i)
\]</span></p>
<p>Defining <span class="math inline">\(p_j = P(X \in L_i)\)</span> and define <span class="math inline">\(\hat{\theta_j}\)</span> as the estimator of <span class="math inline">\(E[Y|x \in L_j]\)</span>. Then, the stratified <em>estimator</em> is:</p>
<p><span class="math display">\[
\hat{\theta} = \sum_{i=j}^k \widehat{E[Y|x \in L_j]} \, p_j= \sum_{j=1}^k \hat{\theta_j} \, p_j
\]</span></p>
<p>In this situation, <span class="math inline">\(p_j\)</span> are known constants, computed in advance. This method replaces uncertainty by computing <span class="math inline">\(p_j\)</span> and estimating <span class="math inline">\(\hat{\theta_j}\)</span>, where <span class="math inline">\(\hat{\theta_j}\)</span> is the sample average of the observations that fall within strata <span class="math inline">\(L_j\)</span>.</p>
<p><span class="math display">\[
p_j = P(X \in L_i) = \int_{b_{j-1}}^{b_j}f(x)dx
\]</span>
In stratified sampling, we can pre-determine how many observations to sample within each strata as part of the design of the sampling plan. There is another form of stratified sampling, that classifies each observation after sampling from the overall population as belonging to strata <span class="math inline">\(L_j\)</span>. This approach is called post-stratified sampling.</p>
<p>In post stratified sampling, we observe the IID pairs <span class="math inline">\((Y_1, X_1), (Y_2, X_2), \cdots ,(Y_n, X_n)\)</span> and form an estimator for <span class="math inline">\(\theta_j\)</span> by computing the observed sample average for the strata. Let <span class="math inline">\(Y_{ij}\)</span> be the <span class="math inline">\(i^{th}\)</span> observation of <span class="math inline">\(Y\)</span> when <span class="math inline">\(X \in L_j\)</span>, for <span class="math inline">\(j=1,2,\cdots,k\)</span> and <span class="math inline">\(i=1,2,\cdots, N_j\)</span>, where <span class="math inline">\(N_j\)</span> is the number of observations of <span class="math inline">\(Y_i\)</span> in the <span class="math inline">\(j^{th}\)</span> strata. Note that <span class="math inline">\(N_j\)</span> is a random variable. The estimator for <span class="math inline">\(\theta_j\)</span> is:</p>
<p><span class="math display">\[
\hat{\theta_j} = \frac{1}{N_j}\sum_{i=1}^{N} Y_{ij}
\]</span></p>
<p>The post stratified estimator is then</p>
<p><span class="math display">\[\hat{\theta} = \sum_{j=1}^{k}\hat{\theta_j}\,p_j\]</span></p>
<p>If <span class="math inline">\(N_j &gt; 0\)</span> for all <span class="math inline">\(j\)</span>, then <span class="math inline">\(E[\hat{\theta_j}] = \theta_j\)</span> and thus <span class="math inline">\(E[\hat{\theta}]=\theta\)</span>. That is, it will be an unbiased estimator.</p>
<p>There are two main issues when applying stratified sampling: 1) how to form the strata, and 2) how many observations should be sampled in each strata. The formation of the strata is often problem dependent. Then, given the chosen strata a sampling plan (i.e.<span class="math inline">\(N_j\)</span> for each strata can be analyzed). The key idea for picking the strata is to try to ensure that the means for each strata are different from the overall mean. Let <span class="math inline">\(\sigma^2_j = var[Y_j |X\in L_j]\)</span> be the within strata variance. One can show that:</p>
<p><span class="math display">\[
\text{Var}(\hat{\theta}) \cong \sum_{j=1}^{k}\frac{p_j \, \sigma^2_j}{n} + \sum_{j=1}^{k}\frac{(1-p_j) \, \sigma^2_j}{n^2}
\]</span></p>
<p>And, that</p>
<p><span class="math display">\[
\text{Var}(\bar{Y}) = \text{Var}(\hat{\theta}) + \sum_{j=1}^{k}\frac{(\theta_j -\theta)^2 \, p_j}{n}
\]</span></p>
<p>Thus, a large spread (difference) for the <span class="math inline">\(j^{th}\)</span> strata mean from the overall mean <span class="math inline">\(\theta\)</span> will cause, <span class="math inline">\(\text{Var}(\bar{Y}) \geq \text{Var}(\hat{\theta})\)</span>. You should also pick the number of strata <span class="math inline">\(k\)</span> to be small such that you can try to guarantee that <span class="math inline">\(n\,p_j \geq 5\)</span> to ensure that <span class="math inline">\(N_j &gt; 0\)</span>.</p>
<p>In the case of stratified sampling, we can control <span class="math inline">\(N_j\)</span>. That is, <span class="math inline">\(N_j\)</span> is no longer random and under our control as <span class="math inline">\(n_j\)</span> for <span class="math inline">\(j=1,2,\cdots,k\)</span>. We pre-plan how much to sample in each <span class="math inline">\(L_j\)</span> via <span class="math inline">\(n_j\)</span>. One can show that in this case the optimal allocation of <span class="math inline">\(n\)</span> that minimizes that variance is:</p>
<p><span class="math display">\[
n_j = \frac{n \, p_j \, \sigma_j}{\sum_{i=1}^{k}p_i\,\sigma_i}
\]</span>
In essence, the minimal variance will occur if <span class="math inline">\(n_j\)</span> is sampled proportional to <span class="math inline">\(p_i\,\sigma_i\)</span>. For further details of these results, see <span class="citation">(<a href="#ref-rubenstein-kroese">Rubinstein and Kroese. 2017</a>)</span> or <span class="citation">(<a href="#ref-ross2023">Ross 2023</a>)</span>. By estimating the within strata variance <span class="math inline">\(\sigma^2_j = \text{Var}[Y_j |X\in L_j]\)</span> via a pilot run a starting value for <span class="math inline">\(n_j\)</span> can be obtained. Another (convenient) approach is to allocate the <span class="math inline">\(n_j\)</span> proportionally as <span class="math inline">\(n_j = n\,p_j\)</span>. As discussed in <span class="citation">(<a href="#ref-rubenstein-kroese">Rubinstein and Kroese. 2017</a>)</span>, the stratified sampling estimator will be unbiased and should have a variance reduction when compared to the crude Monte Carlo (CMC) estimator.</p>
<p>The stratified sampling method within a Monte Carlo context is relatively straight-forward to implement. The KSL utilities package for random sampling and statistical collection can be used in such an implementation. For the case of a DEDS situation, stratified sampling has less direct appeal. Although the post-stratified sampling method could be automated by capturing the response <span class="math inline">\(Y_j\)</span> and generated <span class="math inline">\(X\)</span> from the simulation (within replication data), it is not straight-forward to connect the generated <span class="math inline">\(X\)</span> values with the responses. Thus, the KSL does not provide constructs to specifically implement this variance reduction technique.</p>
</div>
<div id="conditional-expectation-ce" class="section level3 hasAnchor" number="9.2.6">
<h3><span class="header-section-number">9.2.6</span> Conditional Expectation (CE)<a href="ch9VRTs.html#conditional-expectation-ce" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For completeness, this section discusses another less general form of variance reduction technique: conditional expectation. As we will see, this technique is very problem dependent. Thus, this section presents the concepts within the context of a particular example. For more of the theory behind this technique, we refer the interested reader to <span class="citation">(<a href="#ref-rubenstein-kroese">Rubinstein and Kroese. 2017</a>)</span> or <span class="citation">(<a href="#ref-ross2023">Ross 2023</a>)</span>. Like stratified sampling and control variates this method work on using <span class="math inline">\(E[Y|X]\)</span>. Recall <span class="math inline">\(\theta = E[Y]= E\bigg[E[Y|X]\bigg]\)</span>. It can be shown, see <span class="citation">(<a href="#ref-ross2023">Ross 2023</a>)</span>, that <span class="math inline">\(\text{Var}[E[Y|X]] \leq \text{Var}[\bar{Y}]\)</span>. Thus, if we <em>know</em> the function <span class="math inline">\(E[Y|X=x]\)</span> and have a sample <span class="math inline">\(X_1, X_2,\cdots, X_n\)</span>, then we can use:</p>
<p><span class="math display">\[
\hat{\theta} = \frac{1}{n}\sum_{i=1}^{n}E[Y|X_i]
\]</span>
This technique relies on being able to derive the function <span class="math inline">\(E[Y|X=x]\)</span> from the problem situation, which can be very problem dependent. We will demonstrate this via an example involving a stochastic activity network.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:SAN"></span>
<img src="figures2/ch9/SAN.PNG" alt="Example Stochastic Activity Network" width="60%" height="60%" />
<p class="caption">
Figure 9.5: Example Stochastic Activity Network
</p>
</div>
<p>Let <span class="math inline">\(X_i\)</span> be the time required to complete arc <span class="math inline">\(i\)</span>. Assume that the <span class="math inline">\(X_i\)</span> are independent. A path through an activity network is a sequence of arcs going from a source node to a sink node. For the example network, we have three paths.</p>
<ul>
<li>Path 1: <span class="math inline">\(Y_1 = X_1 + X_4\)</span></li>
<li>Path 2: <span class="math inline">\(Y_2 = X_1 + X_3 + X_5\)</span></li>
<li>Path 3: <span class="math inline">\(Y_3 = X_2 + X_5\)</span></li>
</ul>
<p>Activity networks are often simulated in order to observe the longest path,e.g.<span class="math inline">\(Y = max(Y_1, Y_2, Y_3)\)</span> and estimate quantities such as <span class="math inline">\(E[Y]\)</span> and <span class="math inline">\(P(Y \leq t)\)</span>. A crude Monte Carlo algorithm for estimating <span class="math inline">\(E[Y]\)</span> and <span class="math inline">\(P(Y \leq t)\)</span> is as follows.</p>
<ol start="0" style="list-style-type: decimal">
<li>Let <span class="math inline">\(P_j\)</span> be the arcs on path <span class="math inline">\(j\)</span>; choose t; let <span class="math inline">\(c=0\)</span>; <span class="math inline">\(sum = 0\)</span></li>
</ol>
<p>for <span class="math inline">\(n=1\)</span> to <span class="math inline">\(k\)</span>;</p>
<ol style="list-style-type: decimal">
<li>generate <span class="math inline">\(X_i \sim F_i(x)\)</span>, <span class="math inline">\(i=1,2,3,4,5\)</span></li>
<li>compute <span class="math inline">\(Y_j = \sum_{i\in P_j}X_i\)</span>, for <span class="math inline">\(j=1,2,3\)</span></li>
<li>let <span class="math inline">\(Y = max(Y_1, Y_2, Y_3)\)</span></li>
<li>if (<span class="math inline">\(Y \leq t\)</span>) then <span class="math inline">\(c=c+1\)</span>; <span class="math inline">\(sum = sum + Y\)</span></li>
</ol>
<p>end for;</p>
<ol start="5" style="list-style-type: decimal">
<li><span class="math inline">\(\widehat{P(Y \leq t)} = c/k\)</span></li>
<li><span class="math inline">\(\bar{Y} = sum/k\)</span></li>
</ol>
<p>In what follows, we will develop functional form for <span class="math inline">\(E[Y|X=x]\)</span> so that the conditional expectation variance reduction technique can be applied. We begin by developing the distribution for <span class="math inline">\(P(Y \leq t)\)</span>. The distribution of <span class="math inline">\(Y\)</span> the maximum path length is defined as:</p>
<p><span class="math display">\[
P(Y \leq t) = P(max(Y_1, Y_2, Y_3)\leq t)=P(Y_1 \leq t, Y_2 \leq t, Y_3 \leq t)
\]</span></p>
<p>Thus, the <span class="math inline">\(Y_i\)</span> are not independent random variable because they share arcs. This dependence can be exploited for the application of CE. Another way to write <span class="math inline">\(P(Y \leq t)\)</span> can be useful. Define an indicator function <span class="math inline">\(g(X_1, X_2, X_3, X_4, X_5)\)</span> as follows:</p>
<p><span class="math display">\[
g(X_1, X_2, X_3, X_4, X_5)=\begin{cases}
1 &amp; \text{if } \, Y \leq t\\
0 &amp; \text{if } \, Y &gt; t\\
\end{cases}
\]</span>
Now, define <span class="math inline">\(f_i(x_i)\)</span> as the distribution function the length, <span class="math inline">\(X_i\)</span>, of each arc <span class="math inline">\(i\)</span>. Then, the indicator function <span class="math inline">\(g(\cdot)\)</span> can be used to define <span class="math inline">\(P(Y \leq t)\)</span> as follows:</p>
<p><span class="math display">\[
P(Y \leq t) = \int_0^{\infty}\int_0^{\infty}\int_0^{\infty}\int_0^{\infty}\int_0^{\infty}g(X_1, \cdots, X_5)\,f_1(x_1)f_2(x_2)\cdots f_5(x_5)\,dx_1\cdots dx_5
\]</span>
Thus, <span class="math inline">\(P(Y \leq t) = E[g(X_1, \cdots, X_5)]\)</span>. This form <span class="math inline">\(P(Y \leq t) = E[g(X_1, \cdots, X_5)]\)</span> allows us to more easily see the conditional expectation possibilities.</p>
<p>Consider <span class="math inline">\(E[g(X_1, \cdots, X_5)|X_1=x_1, X_5=x_5]\)</span>, which is the conditional expectation of <span class="math inline">\(g(\cdot)\)</span> given arcs one and five. Notice from Figure <a href="ch9VRTs.html#fig:SAN">9.5</a> that <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_5\)</span> occur in each of the three paths. That is, arcs one and five are common on the three paths.</p>
<p>Note that by conditional expectation, we can write <span class="math inline">\(P(Y \leq t) = E_{X_1,X_5}\bigg[E[g(X_1, \cdots, X_5)|X_1, X_5]\bigg]\)</span> and that <span class="math inline">\(E[g(X_1, \cdots, X_5)|X_1=x_1, X_5=x_5]\)</span> can be written as:</p>
<p><span class="math display">\[
\begin{aligned}
E[g(X_1, \cdots, X_5)|X_1=x_1, X_5=x_5] &amp;= P(Y \leq t|X_1=x_1, X_5=x_5)\\
  &amp;= P(Y_1 \leq t, Y_2 \leq t, Y_3 \leq t|X_1=x_1, X_5=x_5)\\
  &amp;= P(X_1 + X_4 \leq t, X_1+X_3+X_5 \leq t, X_2+X_5 \leq t|X_1=x_1, X_5=x_5)\\
  &amp;= P( X_4 \leq t-x_1, X_3 \leq t-x_1-x_5, X_2\leq t-x_5)\\
  &amp;= P( X_4 \leq t-x_1)P(X_3 \leq t-x_1-x_5)P( X_2\leq t-x_5)\\
  &amp;= F_4(t-x_1)F_3(t-x_1-x_5)F_2( t-x_5)\\
\end{aligned}
\]</span>
Notice that <span class="math inline">\(E[g(X_1, \cdots, X_5)|X_1=x_1, X_5=x_5]\)</span> is simply a function of <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_5\)</span>. Thus after observing <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_5\)</span>, we can average over this expression and compute <span class="math inline">\(P(Y \leq t)\)</span>. A simple algorithm for this can be written as.</p>
<ol start="0" style="list-style-type: decimal">
<li><span class="math inline">\(sum = 0\)</span></li>
</ol>
<p>for <span class="math inline">\(n=1\)</span> to <span class="math inline">\(k\)</span>;</p>
<ol style="list-style-type: decimal">
<li>generate <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_5\)</span></li>
<li>sum = sum + <span class="math inline">\(F_4(t-x_1)F_3(t-x_1-x_5)F_2( t-x_5)\)</span></li>
</ol>
<p>end for;</p>
<ol start="4" style="list-style-type: decimal">
<li><span class="math inline">\(\widehat{P(Y \leq t)} = sum/k\)</span></li>
</ol>
<p>The conditional expectation estimator (if found) will guarantee a variance reduction. The amount of variance reduction is problem dependent.</p>
<p>Since this technique is problem dependent, the KSL does not provide general support for applying CE; however, as noted in the simple algorithms that were presented, the KSL support for generating random variates and collecting statistics would be essential in such an implementation. The difficulty of applying CE to static Monte Carlo situations is based on the problem situation but relatively straight-forward if the conditional expectation function can be derived; however, CE has received limited application in the more general DEDS situation.</p>
</div>
<div id="importance-sampling-is" class="section level3 hasAnchor" number="9.2.7">
<h3><span class="header-section-number">9.2.7</span> Importance Sampling (IS)<a href="ch9VRTs.html#importance-sampling-is" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The final variance reduction technique to be discussed is general in nature; however, although it has many applications its application is significant within a specific static Monte Carlo situation, specifically the estimation of the area of a function.</p>
<p><span class="math display">\[
\theta = \int\limits_{a}^{b} g(x) \mathrm{d}x
\]</span>
Recall that the crude or simple Monte Carlo evaluation of an integral utilizes the uniform distribution:</p>
<p><span class="math display">\[
f_{X}(x) =
\begin{cases}
\frac{1}{b-a} &amp; a \leq x \leq b\\
0   &amp; \text{otherwise}
\end{cases}
\]</span></p>
<p>Therefore,</p>
<p><span class="math display">\[
E_{X}\lbrack g(x) \rbrack  = \int\limits_{a}^{b} g(x)f_{X}(x)\mathrm{d}x = \int\limits_{a}^{b} g(x)\frac{1}{b-a}\mathrm{d}x
\]</span>
Defining <span class="math inline">\(Y = \left(b-a\right)g(X)\)</span> and substituting, yields,
<span class="math display">\[
\begin{aligned}
E\lbrack Y \rbrack &amp; = E\lbrack \left(b-a\right)g(X) \rbrack = \left(b-a\right)E\lbrack g(X) \rbrack\\
      &amp; =  \left(b-a\right)\int\limits_{a}^{b} g(x)\frac{1}{b-a}\mathrm{d}x = \int\limits_{a}^{b} g(x)\mathrm{d}x = \theta\end{aligned}
\]</span>
Therefore, by estimating the expected value of <span class="math inline">\(Y\)</span> with the sample average of the <span class="math inline">\(Y_i\)</span> values, we can estimate the desired integral. Importance sampling does this same thing but does not necessarily use the <span class="math inline">\(U(a,b)\)</span> distribution. Importance sampling rewrites the integration as follows:</p>
<p><span class="math display">\[
\theta = \int\limits_{a}^{b} g(x) \mathrm{d}x = \int\limits_{a}^{b}\frac{g(x)}{w(x)}w(x)\mathrm{d}x
\]</span>
The function <span class="math inline">\(w(x)\)</span> is a probability density function defined over <span class="math inline">\([a, b]\)</span>, where <span class="math inline">\(X\sim w(x)\)</span>. Instead of defining <span class="math inline">\(Y = \left(b-a\right)g(X)\)</span>, we define <span class="math inline">\(Y\)</span> as:</p>
<p><span class="math display">\[
Y = \frac{g(X)}{w(X)}
\]</span></p>
<p>Therefore:</p>
<p><span class="math display">\[
E[Y] = E_X\bigg[\frac{g(X)}{w(X)}\bigg]= \int\limits_{a}^{b}\frac{g(x)}{w(x)}w(x)\mathrm{d}x=\int\limits_{a}^{b} g(x) \mathrm{d}x =\theta
\]</span></p>
<p>To estimate <span class="math inline">\(\theta\)</span>, we use <span class="math inline">\(\bar{Y}\)</span>, where <span class="math inline">\(X_i\sim w(x)\)</span> and:</p>
<p><span class="math display">\[
\hat{\theta} = \bar{Y}(n) = \frac{1}{n}\sum_{i=1}^{n}Y_i=\frac{1}{n}\sum_{i=1}^{n}\frac{g(X_i)}{w(X_i)}
\]</span>
A judicious choice of <span class="math inline">\(w(x)\)</span> can reduce the variance of the estimator <span class="math inline">\(\hat{\theta}\)</span> allowing us to get more precision in the same amount of samples. While illustrated for 1-D integration, this same theory applies to higher dimensional integrals. However, higher dimensional integrals requires that <span class="math inline">\(\vec{X} \sim w(\vec{X})\)</span> be a multi-variate distribution.</p>
<p>In general, any function <span class="math inline">\(g(x)\)</span> can be factored into two functions such that <span class="math inline">\(g(x) = h(x)f(x)\)</span> where <span class="math inline">\(f(x)\)</span> is a probability density function. The optimal choice for <span class="math inline">\(w(x)\)</span> is the function <span class="math inline">\(f(x)\)</span>. However, to find this function, we would in general need to know <span class="math inline">\(\theta\)</span>, which is the thing we are trying to estimate.</p>
<p>A good choice for <span class="math inline">\(w(x)\)</span> is one that will cause more samples when <span class="math inline">\(g(x)\)</span> is large and sample infrequently when <span class="math inline">\(g(x)\)</span> is small. That is, sample more in the more important areas. This is the genesis for the name importance sampling. Lets take a look at an example to illustrate these insights.</p>
<p>Suppose we want to integrate the function <span class="math inline">\(g(x) = x^2\)</span> over the range <span class="math inline">\(0\leq x \leq 1\)</span>. This is easy because we can compute the answer:</p>
<p><span class="math display">\[
\theta = \int\limits_{0}^{1} g(x) \mathrm{d}x = \int\limits_{0}^{1}x^2\mathrm{d}x=\frac{x^3}{3}\bigg|_0^1 = \frac{1}{3}
\]</span></p>
<p>The crude MC estimator has the following factorization, <span class="math inline">\(g(x) = h(x)\, f(x)\)</span>, where <span class="math inline">\(h(x) = x^2\)</span> and:</p>
<p><span class="math display">\[
f_{X}(x) =
\begin{cases}
\frac{1}{1-0} &amp; 0 \leq x \leq 1\\
0   &amp; \text{otherwise}
\end{cases}
\]</span></p>
<p>Thus, the crude MC estimator for <span class="math inline">\(\theta = E_f[h(X)]\)</span> is:</p>
<p><span class="math display">\[
\bar{Y}(n) = \frac{1}{n}\sum_{i=1}^{n}h(X_i)=\frac{1}{n}\sum_{i=1}^{n}g(X_i)
\]</span>
Lets derive the variance of the crude MC estimator.</p>
<p><span class="math display">\[
\text{Var}(\bar{Y}(n)) =\frac{1}{n}\text{Var}(g(X_i))=\frac{1}{n}\bigg(E[g^2(X)]-\theta^2\bigg)
\]</span></p>
<p><span class="math display">\[
E[g^2(X)] = \int_0^{1}g^2(x)dx=\int_0^{1}x^4dx=\frac{x^5}{5}\bigg|_0^1 = \frac{1}{5}
\]</span></p>
<p><span class="math display">\[
\text{Var}(\bar{Y}(n)) =\frac{1}{n}\bigg(\frac{1}{5}-\theta^2\bigg) =\frac{1}{n}\bigg(\frac{1}{5}-\frac{1}{9}\bigg)=\frac{1}{n}\bigg(\frac{4}{45}\bigg)
\]</span></p>
<p>We must try to do better than this value using importance sampling. Also, note that in general, we would not know <span class="math inline">\(\theta\)</span> to do this calculation.</p>
<p>The effectiveness of importance sampling hinges on picking the proposal distribution <span class="math inline">\(w(x)\)</span>. The goal is to pick a proposal density <span class="math inline">\(w(x)\)</span> that improves on <span class="math inline">\(\text{Var}(\bar{Y}(n))\)</span> of the crude MC estimator. Consider the following possibility:</p>
<p><span class="math display">\[
w(x) =
\begin{cases}
2x &amp; 0 \leq x \leq 1\\
0   &amp; \text{otherwise}
\end{cases}
\]</span>
Clearly, this <span class="math inline">\(w(x)\)</span> is a probability density function since <span class="math inline">\(\int_0^{1}w(x)dx=\int_0^{1}2x\,dx=\frac{2x^2}{2}\bigg|_0^1 = 1\)</span>. The importance sampling estimator is based on the following:</p>
<p><span class="math display">\[
\hat{\theta} = \bar{Y}(n) = \frac{1}{n}\sum_{i=1}^{n}Y_i=\frac{1}{n}\sum_{i=1}^{n}\frac{g(X_i)}{w(X_i)}
\]</span>
Lets derive the variance of this estimator based on the given function <span class="math inline">\(w(x)\)</span>. Using the definition of variance, we have:</p>
<p><span class="math display">\[
\begin{aligned}
\text{Var}(\hat{\theta}) &amp;=\frac{1}{n}\text{Var}\bigg( \frac{g(X_i)}{w(X_i)}\bigg)=\frac{1}{n}\bigg(E_w\Bigg[\Bigg(\frac{g(X_i)}{w(X_i)}\Bigg)^2\Bigg]-\theta^2\bigg)\\
&amp;=\frac{1}{n}\bigg(   \int_0^{1}\frac{g^2(x)}{w^2(x)}\,w(x) \,dx  -\theta^2\bigg)
\end{aligned}
\]</span></p>
<p>Now, we need to complete this derivation by deriving the second moment for the example problem.</p>
<p><span class="math display">\[
\int_0^{1}\frac{g^2(x)}{w(x)} \,dx=\int_0^{1}\frac{x^4}{2x}\,dx = \int_0^{1}\frac{x^3}{2}\,dx =\frac{x^4}{8}\bigg|_0^1 = \frac{1}{8}
\]</span>
Therefore:</p>
<p><span class="math display">\[
\text{Var}(\hat{\theta}) =\frac{1}{n}\bigg(\frac{1}{8}-\theta^2\bigg) =\frac{1}{n}\bigg(\frac{1}{8}-\frac{1}{9}\bigg)=\frac{1}{n}\bigg(\frac{1}{72}\bigg)
\]</span>
Lets look at the ratio of the variances of the two competing estimators:</p>
<p><span class="math display">\[
VR = \frac{\text{Var}(\hat{\theta})}{\text{Var}(\bar{Y}(n))}=\frac{\frac{1}{n}\bigg(\frac{1}{72}\bigg)}{\frac{1}{n}\bigg(\frac{4}{45}\bigg)}=\frac{4}{3240}&lt; 1
\]</span></p>
<p>There will be a significant variance reduction with this choice of <span class="math inline">\(w(x)\)</span> over using the standard uniform distribution.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ISExample"></span>
<img src="figures2/ch9/ISExample.PNG" alt="Importance Sampling Based on w(x)" width="60%" height="60%" />
<p class="caption">
Figure 9.6: Importance Sampling Based on w(x)
</p>
</div>
<p>Figure <a href="ch9VRTs.html#fig:ISExample">9.6</a> shows the two proposal functions <span class="math inline">\(w_1(x)\)</span> (<span class="math inline">\(U(0,1)\)</span>) and <span class="math inline">\(w_2(x) = 2x\)</span> relative to <span class="math inline">\(g(x)\)</span>. We can see that the closer the proposal distribution is to <span class="math inline">\(g(x)\)</span> as the intuition behind why there is such a dramatic variance reduction.</p>
<p><span class="citation">(<a href="#ref-rubenstein-kroese">Rubinstein and Kroese. 2017</a>)</span> devotes considerable time to the topic of importance sampling. The most interesting aspect of their treatment is the discussion on sequential importance sampling, which dynamically updates the proposal distribution as the sampling procedure proceeds in a manner that tries to converge towards the optimal proposal distribution. We encourage the interested reader to explore the reference for further details.</p>
<p>The KSL supports the application of importance sampling via the <code>MC1DIntegration</code> and <code>MCMultiVariateIntegration</code> classes. Section <a href="mcmExperiments.html#mcmExperiments">3.8</a> described the use of the <code>MC1DIntegration</code>, which permits the application of importance sampling by allowing the user to provide <span class="math inline">\(h(x)\)</span>, where</p>
<ul>
<li><span class="math inline">\(w(x)\)</span> is the probability distribution for the random variable supplied by the sampler,</li>
<li><span class="math inline">\(g(x)\)</span> is the function that needs to be integrated, and</li>
<li><span class="math inline">\(h(x)\)</span> is a factorization of <span class="math inline">\(g(x)\)</span> such that <span class="math inline">\(g(x) = h(x)*w(x)\)</span>, that is <span class="math inline">\(h(x) = g(x)/w(x)\)</span></li>
</ul>
<p>The exercises will ask the reader to apply importance sampling to some of the problems already faced in Chaper <a href="mcm.html#mcm">3</a>. The KSL also supports the application of importance sampling to multi-dimensional integrals; however, since that topic requires the concepts of how to generate from multi-variate distributions, we will defer that discussion until after the next section.</p>
</div>
</div>
<h3><span class="header-section-number">G</span> References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-law2007simulation" class="csl-entry">
Law, A. 2007. <em>Simulation Modeling and Analysis</em>. 4th ed. McGraw-Hill.
</div>
<div id="ref-nelson1990" class="csl-entry">
Nelson, B. L. 1990. <span>Control Variate Remedies.</span> <em>Operations Research</em> 38 (6): 97492.
</div>
<div id="ref-nelson-pei" class="csl-entry">
Nelson, B. L., and L. Pei. 2021. <em>Foundations and Methods of Stochastic Simulation: A First Course</em>. Vol. 316. International Series in Operations Research &amp; Management Science. Springer International Publishing.
</div>
<div id="ref-ross2023" class="csl-entry">
Ross, Sheldon M. 2023. <em>Simulation (6th Edition)</em>. Elsevier.
</div>
<div id="ref-rubenstein-kroese" class="csl-entry">
Rubinstein, R., and D. Kroese. 2017. <em>Simulation and the Monte Carlo Method</em>. John Wiley &amp; Sons Inc.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ch9BootStrapping.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ch9GMVRVs.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
